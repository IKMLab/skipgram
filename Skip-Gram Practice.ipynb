{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing necessary preliminaries...\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import glovar\n",
    "import collections\n",
    "import time\n",
    "from data import toxic\n",
    "toxic.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Implementation Practice\n",
    "\n",
    "Your goal is to implement the Skip-Gram model in PyTorch, including pre-processing. \n",
    "\n",
    "Pre-processing is an important step in deep learning with text, and you should learn it now. If you think you know it well, skip to number (5) and refer to the instructions. \n",
    "\n",
    "This tutorial assumes you are familiar with the basics of PyTorch. If not, you can review some introductory tutorials such as:\n",
    "\n",
    "https://github.com/jcjohnson/pytorch-examples\n",
    "\n",
    "Stages in this tutorial:\n",
    "1. Import the data\n",
    "2. Tokenize the data\n",
    "3. Build the vocab dictionary\n",
    "4. Prepare training pairs\n",
    "5. Implement negative sampling\n",
    "6. Code the model\n",
    "7. Train the model\n",
    "8. Visualize the word embeddings\n",
    "\n",
    "Tips are provided for the pre-processing stage to make it smoother. Try not to use them. If you have to, put aside some time to learn those skills properly.\n",
    "\n",
    "You will see validation cells along the way with `assert` calls. Run these to make sure you haven't made any mistakes along the way that will prevent you from proceeding. You will need to do the steps in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Data\n",
    "\n",
    "We will use the data from Kaggle's Toxic Comment classification task. This data is very noisy, so we have selected a subset of the data that reduces this noise to make this tutorial easier.\n",
    "\n",
    "The files are provided in the `data` directory. They come in .csv format so we'll use pandas to handle them. If you haven't learned how to use pandas, do it! It is a very useful tool.\n",
    "\n",
    "The next cell defines the paths to these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = os.path.join(glovar.DATA_DIR, 'train.csv')\n",
    "test_file_path = os.path.join(glovar.DATA_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "Load both .csv files into pandas `DataFrame` objects called `train` and `test` and see what they look like by calling the `head()` function on one of them. Identify the name of the column that contains the text of the comments. \n",
    "\n",
    "Tips:\n",
    "- If you don't know pandas, try using `pd.read_csv()`, but put aside some time to learn pandas properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = ???\n",
    "# test = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "We need to determine the set of all tokens in our dataset. We therefore need to separate each comment string into individual tokens, then determine the unique set of those tokens. We focus on the tokenization step first.\n",
    "\n",
    "We will use `nltk` for tokenization because it is lightweight. The `nltk` package defines a function called `word_tokenize()` that you can use.\n",
    "\n",
    "### To Do\n",
    "Given the `DataFrame`s above, obtain a `set` of all unique tokens from <strong>both</strong> the train and test sets. We need to account for all the data in our vocabulary. This variable should be called `token_set`.\n",
    "\n",
    "Tips:\n",
    "- The rows of `DataFrame` objects `df1` and `df1` can be joined together using `df1.append(df2)`\n",
    "- You can get an `array` of values in a `DataFrame` `df` column by calling `df['column_name'].values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(token_set) == len(toxic.get_token_set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Vocab Dictionary\n",
    "\n",
    "We need to associate a unique `int` index with every unique token, and provide a map for lookup. A high-level view of text processing is often: \n",
    "1. receive text as input\n",
    "2. tokenize that text to obtain tokens\n",
    "3. use those tokens to obtain integer indices\n",
    "4. use those indices to lookup word vectors\n",
    "5. use those vectors as input to a neural network.\n",
    "\n",
    "We focus on (3) now.\n",
    "\n",
    "### To Do\n",
    "Use the `token_set` to build a `dict` object called `vocab` that has every unique token in the `token_set` as an index and unique integers as `values`. <strong>Also</strong>, since we will add a padding token for recurrent neural network processing later, make sure your values start from `1` and not `0` so that we can keep `0` for padding later.\n",
    "\n",
    "Tips:\n",
    "- The python `zip()` function can be used to bring two lists together - e.g. tokens and indices\n",
    "- The `dict()` constructor can take a zipped object as input, mapping the first position to index and second to value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vocab) == len(toxic.get_vocab())\n",
    "assert isinstance(list(vocab.keys())[0], str)\n",
    "assert isinstance(list(vocab.values())[0], int)\n",
    "assert 0 not in vocab.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a reverse lookup dictionary (integer indexes as keys and string tokens as values) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the Training Pairs\n",
    "\n",
    "We need to present two words at a time to the network to train our Skip-Gram: a center word and a context word. We therefore need to determine these pairs beforehand.\n",
    "\n",
    "Before coding deep learning models it is necessary to first fully think through how we are going to present the data to the network. This will avoid having to make annoying changes that might follow from small details that are easy to overlook.\n",
    "\n",
    "We know we are going to present two words at a time: a center word, and a context word. But how are we going to present them: as tokens, or as indices? These details matter when you code the forward pass of the network: if you try a word vector lookup on an embedding matrix with a string, you will see an error. We will use integer indices as it will be slightly faster than adding a dictionary lookup as well at training time.\n",
    "\n",
    "Since finding the context tokens for all words over all instances in the dataset is not a generally useful skill, we do that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5  # our context window size - experiment with this!\n",
    "contexts = toxic.get_contexts(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contexts` variable is a dictionary where the keys are the indices of all the tokens in the dataset, and the values are `set`s of token indices that occur in their contexts. We will sample from these during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the frequencies of our words for the negative sampling algorithm.\n",
    "\n",
    "### To Do\n",
    "\n",
    "Use the training data and `nltk.word_tokenize` to create a `collections.Counter` object that holds each unique token as a key, and the frequency count as a value.\n",
    "\n",
    "Tips:\n",
    "- `Counter` has an `update()` function that can accept lists of key values (i.e. tokens) and automatically does the counting for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(frequencies) == len(vocab)\n",
    "assert isinstance(list(frequencies.keys())[0], str)\n",
    "assert isinstance(list(frequencies.values())[0], int)\n",
    "rev_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Negative Sampling\n",
    "\n",
    "If you are skipping pre-processing, uncomment the next cell.\n",
    "\n",
    "To perform negative sampling, we need a function that\n",
    "- Takes a token index as argument\n",
    "- Returns the number of negative samples we desire\n",
    "- Randomly chooses those samples according to\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=0}^n (f(w_j)^{3/4})}\n",
    "$$\n",
    "\n",
    "We will define this function as a callable class, since it depends on state information (the `vocab`, `frequencies`, and `contexts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = toxic.get_vocab()\n",
    "frequencies = toxic.get_frequencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    \n",
    "    def __init__(self, vocab, frequencies, contexts):\n",
    "        \"\"\"Create a new NegativeSampler.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary.\n",
    "          frequencies: List of integers, the frequencies of each word,\n",
    "            sorted in word index order.\n",
    "          contexts: Dictionary.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)\n",
    "        self.contexts = contexts\n",
    "        self.distribution = self.p(list(frequencies.values()))\n",
    "    \n",
    "    def __call__(self, tok_ix, num_negs):\n",
    "        \"\"\"Get negative samples.\n",
    "        \n",
    "        Args:\n",
    "          tok_ix: Integer, the index of the center word.\n",
    "          num_negs: Integer, the number of negative samples to take.\n",
    "        \"\"\"\n",
    "        samples = np.random.choice(\n",
    "            self.n, \n",
    "            size=num_negs, \n",
    "            p=self.distribution)\n",
    "        # make sure we haven't sampled center word or its context\n",
    "        invalid = [-1, tok_ix] + list(self.contexts[tok_ix])\n",
    "        for i, ix in enumerate(samples):\n",
    "            if ix in invalid:\n",
    "                while new_ix in invalid:\n",
    "                    new_ix = random.choice(self.n, \n",
    "                                           num_negs, \n",
    "                                           self.distribution)\n",
    "                samples[i] = new_ix\n",
    "        return samples                \n",
    "    \n",
    "    def p(self, freqs):\n",
    "        \"\"\"Determine the probability distribution for negative sampling.\n",
    "        \n",
    "        Args:\n",
    "          freqs: List of integers.\n",
    "        \n",
    "        Returns:\n",
    "          numpy.array.\n",
    "        \"\"\"\n",
    "        \"\"\" Impelement Me\"\"\"\n",
    "        freqs = np.array(freqs)\n",
    "        return np.power(freqs, 3/4) / np.sum(np.power(freqs, 3/4))\n",
    "\n",
    "sampler = NegativeSampler(vocab, frequencies, contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our negative sampler to see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative samples for \"friend\":\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'new_ix' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-080ede9ca4fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrev_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'friend'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-080ede9ca4fb>\u001b[0m in \u001b[0;36msample\u001b[0;34m(token)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Try a different token, that one is not in the vocab.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Negative samples for \"%s\":'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrev_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-7a0a19b446c5>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tok_ix, num_negs)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0;32mwhile\u001b[0m \u001b[0mnew_ix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minvalid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m                     new_ix = random.choice(self.n, \n\u001b[1;32m     34\u001b[0m                                            \u001b[0mnum_negs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'new_ix' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def sample(token):\n",
    "    if token not in vocab.keys():\n",
    "        raise ValueError('Try a different token, that one is not in the vocab.')\n",
    "    print('Negative samples for \"%s\":' % token)\n",
    "    for sample in sampler(vocab[token], 5):\n",
    "        print('\\t%s' % (rev_vocab[sample]))\n",
    "    \n",
    "sample('friend')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code the Model\n",
    "\n",
    "Now for the fun part.\n",
    "\n",
    "Notice that for all the fancy neural network diagrams you see in word2vec tutorials, when you get down to the maths it is just picking vectors out of matrices and calculating their dot products? It's nothing to be frightened of. Just follow the equations closely and it will work. We don't even need to use any complicated functions or neural network modules. In fact, we could do it in numpy (but then we would have to calculate the gradients manually). PyTorch makes this very easy.\n",
    "\n",
    "Below is a template for the model. You need to implement the functions that say `### Impelement Me ###`. They should be self explanatory from the slides/notes. Refer to them if stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"SkipGram Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, emb_dim, num_negs, lr):\n",
    "        \"\"\"Create a new SkipGram.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary, our vocab dict with token keys and index values.\n",
    "          emb_dim: Integer, the size of word embeddings.\n",
    "          num_negs: Integer, the number of non-context words to sample.\n",
    "          lr: Float, the learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)  # size of the vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        ### Implement Me: Initialize self.U and self.V,      ###\n",
    "        ### the parameter matrices of the network.           ###\n",
    "        ### They need to be nn.Parameter objects.            ###\n",
    "        ### V should have word vectors as row matrices,      ###\n",
    "        ### U should have word vectors as column matrices.   ###\n",
    "        ### Use nn.init.uniform() to perform random uniform  ###\n",
    "        ### initialization in the range [-0.01, 0.01].       ###\n",
    "            \n",
    "        self.U = nn.Parameter(torch.Tensor(n, emb_dim), requires_grad=True)\n",
    "        self.V = nn.Parameter(torch.Tensor(emb_dim, n), requires_grad=True)\n",
    "        nn.init.uniform(self.V.weight, a=-0.01, b=0.01)\n",
    "        nn.init.uniform(self.U.weight, a=-0.01, b=0.01)        \n",
    "        \n",
    "        ### End Implement ##\n",
    "        \n",
    "        # Adam is a good optimizer and will converge faster than SGD\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)  \n",
    "    \n",
    "    def forward(self, center_ixs, context_ixs):\n",
    "        \"\"\"Compute the forward pass of the network.\n",
    "        \n",
    "        1. Lookup embeddings for center and context words\n",
    "            - use self.lookup()\n",
    "        2. Sample negative samples\n",
    "            - use self.negative_samples()\n",
    "        2. Calculate the probability estimates\n",
    "            - make sure to implement and use self.softmax()\n",
    "        3. Calculate the loss\n",
    "            - using self.loss()\n",
    "        \n",
    "        Args:\n",
    "          center_ixs: List of integer indices.\n",
    "          context_ixs: List of integer indices.\n",
    "        \n",
    "        Returns:\n",
    "          loss (torch.autograd.Variable).\n",
    "        \"\"\"\n",
    "        ### Implement Me ###\n",
    "        center_words = self.embedding_lookup(self.V, center_ixs)\n",
    "        context_words = self.embedding_lookup(self.U, context_ixs)\n",
    "        negatives = self.negative_samples(center_words)\n",
    "        context_plus_negs = torch.stack([context_words, negatives])\n",
    "        logits = center_words.matmul(context_plus_negs)\n",
    "        preds = self.softmax(logits)\n",
    "        targets = torch.LongTensor(context_ixs)\n",
    "        return self.loss(preds, targets)\n",
    "    \n",
    "    def lookup(embedding, indices):\n",
    "        \"\"\"Lookup embeddings given indices.\n",
    "        \n",
    "        Args:\n",
    "          embedding: nn.Parameter, an embedding matrix.\n",
    "          indices: List of integers, the indices to lookup.\n",
    "        \n",
    "        Returns:\n",
    "          torch.autograd.Variable of shape [len(indices), emb_dim]. A matrix \n",
    "            with horizontally stacked word vectors.\n",
    "        \"\"\"\n",
    "        lookup_tensor = torch.LongTensor(indices)\n",
    "        return embedding[lookup_tensor]\n",
    "    \n",
    "    def loss(self, preds, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\n",
    "        \n",
    "        Implement this for practice, don't use the built-in PyTorch function.\n",
    "        \n",
    "        Args:\n",
    "          preds: Tensor of shape [batch_size, vocab_size], our predictions.\n",
    "          targets: List of integers, the vocab indices of target context words.\n",
    "        \"\"\"\n",
    "        ### Implement Me ###\n",
    "        return -1 * torch.sum(targets * torch.log(preds))\n",
    "    \n",
    "    def optimize(self, loss):\n",
    "        \"\"\"Optimization step.\n",
    "        \n",
    "        Args:\n",
    "          loss: Scalar.\n",
    "        \"\"\"\n",
    "        # We first make sure to remove any previous gradient from our tensors\n",
    "        # before calculating again.\n",
    "        self.optimizer.zero_grad()\n",
    "        ### Implement Me ###\n",
    "        loss.backward()\n",
    "        self.optimizer.step()        \n",
    "    \n",
    "    def softmax(self, logits):\n",
    "        \"\"\"Compute the softmax function.\n",
    "        \n",
    "        Implement this for practice, don't use the built-in PyTorch function.\n",
    "        \n",
    "        Args:\n",
    "          logits: Tensor of shape [batch_size, vocab_size].\n",
    "        \n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, vocab_size], our predictions.\n",
    "        \"\"\"\n",
    "        ### Impelement Me ###\n",
    "        return torch.exp(logits) / torch.sum(torch.exp(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py35)",
   "language": "python",
   "name": "conda_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
