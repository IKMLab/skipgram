{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from data import toxic\n",
    "import os\n",
    "import glovar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Implementation Practice\n",
    "\n",
    "Your goal is to implement the Skip-Gram model in PyTorch, including pre-processing. Pre-processing is an important step in deep learning with text, and you should learn it now. This tutorial assumes you are familiar with the basics of PyTorch. If not, you can review some introductory tutorials such as:\n",
    "\n",
    "https://github.com/jcjohnson/pytorch-examples\n",
    "\n",
    "Steps in this tutorial:\n",
    "1. Import the data\n",
    "2. Tokenize the data\n",
    "3. Build the vocab dictionary\n",
    "4. Prepare training pairs\n",
    "5. Implement negative sampling\n",
    "6. Code the model\n",
    "7. Train the model\n",
    "8. Visualize the word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the Data\n",
    "\n",
    "We will use the data from Kaggle's Toxic Comment classification task. Head over to\n",
    "\n",
    "https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge\n",
    "\n",
    "and sign up for an account if you don't have one. Download the data and unzip it into the /data folder in the root directory of this project.\n",
    "\n",
    "The files come in .csv format, so we'll use pandas to handle them. If you haven't learned how to use pandas, do it! It is a very useful tool.\n",
    "\n",
    "Make sure, once again, you have unzipped the files into the correct folder (or the code in the next cell will tell you the file is not found)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = os.path.join(glovar.DATA_DIR, 'train.csv')\n",
    "test_file_path = os.path.join(glovar.DATA_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file paths are ready to go. Now load the .csv files for both training and testing into pandas `DataFrame` objects and see what they look like with the `head()` function. Identify the column that contains the text we want to use in the next step, tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(train_file_path)\n",
    "test = pd.read_csv(test_file_path)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "The text data comes as separate strings for each data point, but we need to break each string into tokens, and from the resulting lists determine the set of unique tokens. First tokenize, then determine the set of unique tokens.\n",
    "\n",
    "We will use nltk for tokenization because it is lightweight. The nltk package defines a function `word_tokenize()` that you can use.\n",
    "\n",
    "There are many ways to go from our `DataFrame` objects to a `set` of unique tokens. It is up to you how to do it.\n",
    "\n",
    "At the end of this step you should have a `set` of tokens in a variable called `token_set`. The cell after the next one will then check to make sure you have done this correctly.\n",
    "\n",
    "If you are having problems check step (1) and make sure you have all the data. Make sure you are taking a set and not including any token multiple times. Use the assert conditions to further troubleshoot any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "571120\n"
     ]
    }
   ],
   "source": [
    "token_set = set([])\n",
    "for _, row in train.iterrows():\n",
    "    token_set.update(nltk.word_tokenize(row['comment_text']))\n",
    "for _, row in test.iterrows():\n",
    "    token_set.update(nltk.word_tokenize(row['comment_text']))\n",
    "print(len(token_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explode.',\n",
       " 'Verein',\n",
       " 'title=KZ_Auschwitz-Birkenau',\n",
       " 'yryy4',\n",
       " \"'heros\",\n",
       " 'plottin',\n",
       " 'asterisked',\n",
       " 'this.Thanks',\n",
       " 'Inter-Island',\n",
       " 'Subphylum',\n",
       " 'sucks.==',\n",
       " 'Dine-In',\n",
       " '0945ojtg0o8reuyjurfi',\n",
       " 'baiting/poking',\n",
       " 'CUSTOMSIG',\n",
       " 'FANGUSH',\n",
       " 'Gotlanders',\n",
       " 'raycen',\n",
       " 'colspan=6|',\n",
       " 'secertly',\n",
       " 'KaNyamazane',\n",
       " '=5/1/2011',\n",
       " 'humdrum',\n",
       " '==Characters',\n",
       " 'Legaleze',\n",
       " 'investigations/Davebrayfb',\n",
       " 'pompus',\n",
       " 'pacified',\n",
       " 'textIVE',\n",
       " \"O'Fat\",\n",
       " 'Lucaspet',\n",
       " 'unintuitive',\n",
       " 'interrupter',\n",
       " 'dis-inviting',\n",
       " 'Phyllis',\n",
       " 'overwork',\n",
       " '//www.bakersfield.com/893/story/635460.html',\n",
       " 'Jagira',\n",
       " 'Sumlin',\n",
       " 'TYPICAL',\n",
       " 'White-necked',\n",
       " 'ROAS',\n",
       " 'Writs',\n",
       " 'mid-20th',\n",
       " 'william',\n",
       " 'lobbiest',\n",
       " 'familias',\n",
       " 'Universiade',\n",
       " 'Biomimetic',\n",
       " '2,180',\n",
       " 'cok',\n",
       " \"'smile\",\n",
       " 'nda',\n",
       " 'investigations/Rtuftrbsee',\n",
       " 'ythat',\n",
       " 'nievity',\n",
       " 'Jarvey2',\n",
       " 'Laptop',\n",
       " '3.nm1raines/raycen',\n",
       " 'Gnjilane',\n",
       " 'Colombiano',\n",
       " 'IVOLUME',\n",
       " 'Cost-benefit',\n",
       " 'R/S',\n",
       " 'Barrowlands',\n",
       " 'offender',\n",
       " 'Veljko',\n",
       " 'Socialism',\n",
       " 'dogwood',\n",
       " 'uP',\n",
       " 'Petre',\n",
       " 'Numido-carthagian',\n",
       " '//www.lydiacanaan.com',\n",
       " 'Hachikō',\n",
       " 'AIGA',\n",
       " 'sahabat',\n",
       " 'azınlık',\n",
       " '**Amos',\n",
       " 'smexy',\n",
       " 'a·del·phos′',\n",
       " 'excellencies',\n",
       " '1921-1922',\n",
       " 'Kaluga',\n",
       " 'Shakhlin',\n",
       " 'கொடுக்கிறார்',\n",
       " 'ruslar',\n",
       " 'tyrosine',\n",
       " 'Misessus',\n",
       " 'Mk',\n",
       " 'doingafter',\n",
       " 'MEGO',\n",
       " 'sours',\n",
       " 'negahi',\n",
       " 'Expressiveness',\n",
       " 'Canada.',\n",
       " \"'Kumdo\",\n",
       " 'ዊኪፔዲያ',\n",
       " 'Sofyan',\n",
       " '1-7',\n",
       " 'Exhaust',\n",
       " 'infamnimi',\n",
       " 'thingmebobs',\n",
       " 'Spoc',\n",
       " 'Voyage',\n",
       " 'AMUCHASTEGUI',\n",
       " '01:02',\n",
       " 'agree.Uncreated',\n",
       " '16830',\n",
       " 'riječi',\n",
       " 'Simmalar',\n",
       " 'Reddi',\n",
       " 'PRESENTATIONS',\n",
       " 'PainMan',\n",
       " 'Double-page',\n",
       " 'Hasmonean',\n",
       " 'Dissection',\n",
       " 'free-licensed',\n",
       " 'NotnotKenny',\n",
       " 'Razavi',\n",
       " 'wellstone',\n",
       " 'redlinekd',\n",
       " 'biggee',\n",
       " 'W_Eagle.jpg',\n",
       " 'Intial',\n",
       " 'Ouzelwellians',\n",
       " 'cloistered',\n",
       " 'todaycongratulations',\n",
       " 'fan-led',\n",
       " 'Manticipation',\n",
       " 'Paist',\n",
       " 'slow',\n",
       " 'Attributing',\n",
       " 'Siddhi',\n",
       " 'hand-held',\n",
       " 'Do-Right',\n",
       " 'MARADONA',\n",
       " 'anywaqy',\n",
       " 'tausend',\n",
       " 'YGM',\n",
       " '6735042',\n",
       " 'kirkpatrick',\n",
       " 'Dehloran',\n",
       " 'Testimony',\n",
       " \"'adv\",\n",
       " 'postojati',\n",
       " 'muwahaha',\n",
       " 'möchte',\n",
       " 'poorly.—•',\n",
       " 'Dichter',\n",
       " 'vzdelani',\n",
       " 'ramblesomeI',\n",
       " '12:14',\n",
       " 'Telegraph|date=2',\n",
       " 'Seção',\n",
       " '==bloody',\n",
       " 'lincity-',\n",
       " 'cow-slaughter',\n",
       " '==Toolson',\n",
       " 'αρχάριος',\n",
       " \"'Sources\",\n",
       " 'tildelt',\n",
       " 'tyco',\n",
       " 'Снилось',\n",
       " 'www.fort-plank.com',\n",
       " 'SLACK',\n",
       " 'Noterat',\n",
       " 'Macross',\n",
       " 'Deschampsia',\n",
       " 'ΕΥΤΥΧΙΑ',\n",
       " 'मध्येको',\n",
       " 'dioxin',\n",
       " '✄TAB',\n",
       " 'U*R*G*E*N*T',\n",
       " 'FLOPPIN',\n",
       " 'Castaneda',\n",
       " '*Emphasizes',\n",
       " 'vedere',\n",
       " 'UVITOR',\n",
       " 'reveling',\n",
       " 'Bayh',\n",
       " 'document.getElementById',\n",
       " 'concact',\n",
       " 'ਕਉ',\n",
       " 'ಓದಿ',\n",
       " 'Institute.http',\n",
       " '0.92',\n",
       " 'turn-based',\n",
       " 'panehesy',\n",
       " 'KKK.svg',\n",
       " 'Treki',\n",
       " 'آخری',\n",
       " 'parameters',\n",
       " 'izmešao',\n",
       " 'Wallach==',\n",
       " 'EarthLink',\n",
       " 'usre',\n",
       " 'NintendoWizard22',\n",
       " 'sh**oad',\n",
       " 'debbie',\n",
       " 'Eerste',\n",
       " 'osmanlı',\n",
       " 'Dauber',\n",
       " 'jumpsuit',\n",
       " 'Ex-Nintendo',\n",
       " '//espn.go.com/blog/ncfnation/post/_/id/23051/pac-10-makes-announcement-on-colorado',\n",
       " \"'Crazy\",\n",
       " 'Pentawing',\n",
       " '*.timothyallen.blogs.bbcearth.com',\n",
       " 'Mohammedan',\n",
       " 'brian',\n",
       " 'Boroujerdi',\n",
       " 'Media-ready',\n",
       " 'Nubia',\n",
       " '//en.wikipedia.org/wiki/BloodRockz',\n",
       " 'ecologists',\n",
       " 'oferim',\n",
       " 'Nvidia',\n",
       " 'Aravan',\n",
       " 'Wixer',\n",
       " 'cosinces',\n",
       " 'اليابوسيون',\n",
       " '=31677002',\n",
       " 'forensiscs',\n",
       " '//www.sfbaytimes.com/index.php',\n",
       " 'Biatlo',\n",
       " 'Dave',\n",
       " 'Umer',\n",
       " '1Km',\n",
       " 'ToddlerMommy',\n",
       " 'millenary',\n",
       " 'EIREANN',\n",
       " 'whisked',\n",
       " '1:05',\n",
       " 'white-boy',\n",
       " 'Forgotten',\n",
       " 'değeri',\n",
       " 'Orbelos',\n",
       " 'tool—partedUtil—is',\n",
       " 'Allais',\n",
       " \"'penultimate\",\n",
       " 'मग',\n",
       " 'Profane',\n",
       " 'CrossCountry',\n",
       " 'freiteg.115.242.110.62',\n",
       " '9/11..',\n",
       " 'InedibleHulk',\n",
       " 'cKiTy',\n",
       " 'Wright==',\n",
       " 'Simonian',\n",
       " 'DENIALIST',\n",
       " 'Lagos',\n",
       " 'kosovothanksyou.com',\n",
       " 'Boiled',\n",
       " 'disruptiveor',\n",
       " 'scooter',\n",
       " '2Bwikipedia',\n",
       " 'Necrophilia',\n",
       " 'Hooperswim',\n",
       " 'chartmaker',\n",
       " 'Mongface',\n",
       " 'Sarod',\n",
       " 'wypedzony==',\n",
       " 'artard',\n",
       " 'COIBot/LinkReports/Link',\n",
       " 'Cvijic',\n",
       " 'onlineworldofwrestling',\n",
       " 'CARCASS',\n",
       " 'Towne',\n",
       " 'maqueta',\n",
       " 'sparing',\n",
       " 'NopeI',\n",
       " '203.192.91.4',\n",
       " 'Reductio',\n",
       " 'اصيلة',\n",
       " 'Bidisha',\n",
       " 'fortresses',\n",
       " 'gooey',\n",
       " 'divergance',\n",
       " 'Viking',\n",
       " 'Blundered',\n",
       " 'clumped',\n",
       " 'برقراري',\n",
       " '159.118.158.122',\n",
       " ':whereas',\n",
       " 'Cocking',\n",
       " '//www.leesburgtoday.com/current.cfm',\n",
       " 'Fulner',\n",
       " 'Kruger/Dunning',\n",
       " '|US=yes',\n",
       " 'misljenja-treba',\n",
       " 'folk.',\n",
       " 'می\\u200cرسه',\n",
       " 'OFenian',\n",
       " 'DTD-a',\n",
       " 'izostavljao',\n",
       " 'ommi',\n",
       " 'Germanization',\n",
       " 'gold-leaf',\n",
       " '25/7',\n",
       " 'adjuta',\n",
       " 'cismales',\n",
       " 'PopMatters',\n",
       " 'Rifat',\n",
       " 'irrigated',\n",
       " '//7seals.blogspot.com',\n",
       " 'Matrimony',\n",
       " 'Mahton',\n",
       " 'Kinesiology',\n",
       " 'Scottums',\n",
       " 'Rings==',\n",
       " '//i255.photobucket.com/albums/hh129/reeftee/75246People-_Girls-_Hot_Blonde_In_B.jpg',\n",
       " 'igen',\n",
       " '//moonglow.nu/gardenia/images/mana/mana181.jpg',\n",
       " 'JustAnotherWuBanga',\n",
       " '==SerboCroat',\n",
       " 'Jacqui',\n",
       " 'khiron1416',\n",
       " 'time/intent',\n",
       " 'BAREK',\n",
       " 'flat…',\n",
       " 'Antenna',\n",
       " 'captive-bred',\n",
       " 'unblock|1',\n",
       " '+200',\n",
       " 'sluggish',\n",
       " 'HARM',\n",
       " ':Anyhow',\n",
       " 'attributability',\n",
       " 'Heloo',\n",
       " '5,000,001',\n",
       " 'Southsea',\n",
       " 'garrisons',\n",
       " 'Ruth-2013',\n",
       " 'Betacommand',\n",
       " 'loooong',\n",
       " 'Suggestive',\n",
       " 'anti-wikipedian',\n",
       " 'ଓତିଶା',\n",
       " 'Seitennummerierung',\n",
       " 'Toegang',\n",
       " 'Matthewmayer',\n",
       " 'BCM',\n",
       " 'Unenrolled',\n",
       " 'RRK',\n",
       " 'Helyén',\n",
       " 'Politians',\n",
       " '97',\n",
       " 'SaGa',\n",
       " 'Rome',\n",
       " 'SUPERPOWER..',\n",
       " 'painters',\n",
       " 'Nardwuar',\n",
       " 'laevi',\n",
       " 'Ivester',\n",
       " 'fweddy',\n",
       " '0.0000045',\n",
       " 'ranks',\n",
       " 'byes',\n",
       " 'Rollsroyce',\n",
       " 'Bucketsofg',\n",
       " 'heads..',\n",
       " 'pinoys',\n",
       " 'قران',\n",
       " 'Magyarab',\n",
       " 'fe',\n",
       " 'Mittlöhner',\n",
       " 'sake',\n",
       " 'sharmila-rajesh',\n",
       " 'Yimthirr',\n",
       " 'এইট',\n",
       " 'Arrrrgh',\n",
       " 'gOD',\n",
       " 'HDTVs',\n",
       " 'کودک',\n",
       " '*SmarterChild',\n",
       " 'century—updating',\n",
       " 'Mattiachi',\n",
       " '14.Qxa6',\n",
       " 'commissions',\n",
       " 'ft.',\n",
       " 'MOSAR',\n",
       " 'ELECTRO-MAGNETIC',\n",
       " 'faggot==',\n",
       " 'پناهندگی',\n",
       " 'cquote|Some',\n",
       " 'unfolded',\n",
       " 'ples',\n",
       " 'தாய்',\n",
       " 'Cudgel',\n",
       " 'injests',\n",
       " 'hhave',\n",
       " 'disabled',\n",
       " 'over-involved',\n",
       " 'Tru-cut',\n",
       " 'conventions/facts',\n",
       " 'topic=',\n",
       " 'Xicheng',\n",
       " 'erm',\n",
       " 'Taoist',\n",
       " 'pathedic',\n",
       " 'PENISBIRD',\n",
       " 'vandalism-en-wp',\n",
       " 'sensuousness',\n",
       " 'that.209.162.236.195',\n",
       " 'English/Pseudo-Latin',\n",
       " 'SS',\n",
       " 'ISraeli',\n",
       " 'Nederlandsche',\n",
       " 'Laval',\n",
       " 'formalities',\n",
       " 'figurines',\n",
       " 'alsos',\n",
       " 'osteopathic',\n",
       " 'kamerad',\n",
       " 'cashyy.boyy',\n",
       " 'VEGE',\n",
       " 'Erkenntnis',\n",
       " '**Not',\n",
       " 'Mont-Tremblant',\n",
       " 'Clownfish',\n",
       " 'Ralston',\n",
       " '*against*',\n",
       " '*kiss*sorry',\n",
       " '//www.daintreerainforest.com',\n",
       " 'SM.',\n",
       " 'μάθης',\n",
       " 'Profano',\n",
       " 'evisceration',\n",
       " 'Faukman/Kaufman',\n",
       " '==Pink',\n",
       " \"'Seen\",\n",
       " 'www2.megosave2.tk',\n",
       " 'w.e',\n",
       " 'duger',\n",
       " 'creation/lady',\n",
       " 'Striped',\n",
       " '455,030',\n",
       " 'toenails',\n",
       " 'Parameswara',\n",
       " 'chileans',\n",
       " 'rum.',\n",
       " 'Nkansahrexford',\n",
       " '-Tobit',\n",
       " 'acritically',\n",
       " 'shareef',\n",
       " \"//www.thisislondon.co.uk/news/article-23380710-details/PM's+wife+swops+best+friend+for+a+bisexual+Druid+priestess/article.do\",\n",
       " '//music.mnet.com/Chart/Chart_Genre.asp',\n",
       " 'such..wikipedia',\n",
       " 'Fenianism',\n",
       " 'Oke',\n",
       " 'prem',\n",
       " '20:40',\n",
       " 'p=0.000000001',\n",
       " '//home.hetnet.nl/~motinni/Illuminatus',\n",
       " 'Hrushevskyi',\n",
       " 're-used',\n",
       " 'contribuiu',\n",
       " '1,000lb',\n",
       " 'estimates',\n",
       " '因为欧洲的探险家和中国的宦官有着根本的不同',\n",
       " 'perjorative',\n",
       " 'Gallia',\n",
       " 'defaulting',\n",
       " 'stuck-up',\n",
       " '==Applications',\n",
       " '==TAYLOR',\n",
       " 'Dincher',\n",
       " 'रुझान',\n",
       " 'Арбитражного',\n",
       " 'half-coherent',\n",
       " '0=1',\n",
       " 'doctionary',\n",
       " 'national-football-teams.com',\n",
       " 'foote',\n",
       " 'prática',\n",
       " '//www.chomsky.info/interviews/1990.htm',\n",
       " 'nau',\n",
       " 'applicate',\n",
       " 'Glowing',\n",
       " 'Canaveral',\n",
       " 'Higaonna',\n",
       " 'seal.jpg',\n",
       " 'format',\n",
       " 'Ushuaia',\n",
       " 'Latics',\n",
       " '1901-01-21',\n",
       " '192-206',\n",
       " 'Bramin',\n",
       " '//blog.myspace.com/ksarith',\n",
       " 'bleeding-edge',\n",
       " 'Bombs',\n",
       " 'بول',\n",
       " 'oathed',\n",
       " 'MEMORY',\n",
       " 'وحقيرة',\n",
       " 'Whac-A-Mole',\n",
       " 'Coetzer',\n",
       " 'safe==',\n",
       " 'Gazimestan',\n",
       " 'USCC',\n",
       " '|title=Sean',\n",
       " 'F*-M89',\n",
       " 'Geschaeftsmann',\n",
       " 'Popplewell',\n",
       " 'swathe',\n",
       " 'maimoona',\n",
       " 'Galilean',\n",
       " '//outsourcingandbangladesh.blogspot.comwikt',\n",
       " 'Non-Npov',\n",
       " 'Jassan',\n",
       " 'হাজারটা',\n",
       " 'לְמִינֵהוּ',\n",
       " 'Anyways',\n",
       " 'Albanian',\n",
       " 'grip',\n",
       " '==Religious',\n",
       " '아니라는',\n",
       " 'Mediation_Cabal',\n",
       " 'yuan',\n",
       " 'Enroute',\n",
       " 'Riverfront',\n",
       " 'SUGE',\n",
       " 'resemalance',\n",
       " 'Durante',\n",
       " '==Dummy',\n",
       " 'Firemind',\n",
       " 'Zeitgründen',\n",
       " 'RSVP',\n",
       " 'consensiones',\n",
       " '|1°',\n",
       " 'EXTRATROPICAL',\n",
       " '12000kg',\n",
       " 'Dymond',\n",
       " 'Queue',\n",
       " 'EZTV',\n",
       " 'Actium',\n",
       " 'Presdent',\n",
       " '//www.huffingtonpost.ca/2013/11/22/lilly-singh-superwoman-youtube_n_4324905.html',\n",
       " 'non-terrorist',\n",
       " 'प्रशिद',\n",
       " 'lion-baiting',\n",
       " 'Eistein',\n",
       " 'UGG',\n",
       " '//www.pio.gov.cy/mof/cystat/statistics.nsf/All/5B26588F9EB90E4DC225753E003449AF/',\n",
       " 'Ayvens',\n",
       " 'firoirfjriyhtuiyhtir1fruihrejkyhrnugjvkikmmmmna',\n",
       " 'duties',\n",
       " '|field=micro',\n",
       " 'under-referenced',\n",
       " 't=',\n",
       " 'candidate—see',\n",
       " 'zwischen',\n",
       " 'Mogadishu-Somalia',\n",
       " 'wiley',\n",
       " 'kiki',\n",
       " 'approperiate',\n",
       " 'views.206.124.6.222',\n",
       " '155:160–166',\n",
       " 'Equipo',\n",
       " 'پوشي',\n",
       " 'RVA',\n",
       " 'snares',\n",
       " 'PC-8',\n",
       " 'भ्रातान',\n",
       " 'equivilant',\n",
       " 'wean',\n",
       " 'Sumter',\n",
       " 'v=LAhA-Yd5Nl0',\n",
       " 'Croatiae',\n",
       " 'e-Reader',\n",
       " 'turky',\n",
       " 'good-faith',\n",
       " 'arount',\n",
       " 'H.L',\n",
       " 'debutted',\n",
       " 'Scottywong',\n",
       " 'Jayjjg',\n",
       " 'debris',\n",
       " 'Vanna',\n",
       " 'Khaldoun',\n",
       " 'semiconductors',\n",
       " 'unprofessionalism',\n",
       " \"'transcluded\",\n",
       " '1855-1928.',\n",
       " 'Christian-propaganda',\n",
       " 'opening-heading',\n",
       " '*spastic',\n",
       " 'Nifong',\n",
       " 'purpouse',\n",
       " 'مناطق',\n",
       " 'first-round',\n",
       " 'TOP',\n",
       " '.Thats',\n",
       " 'intertidal',\n",
       " 'Alucards',\n",
       " 'Mi-',\n",
       " '==semi-protect==',\n",
       " 'martinson',\n",
       " 'tipping',\n",
       " '1328',\n",
       " 'Piza',\n",
       " 'මුන්',\n",
       " 'mbéal',\n",
       " 'winches',\n",
       " 'سلام،',\n",
       " 'saif',\n",
       " 'n-word.',\n",
       " 'search-and-replace',\n",
       " 'Shanti',\n",
       " 'Podolksky',\n",
       " 'Lokshin',\n",
       " 'единого',\n",
       " 'Cryptozoology',\n",
       " 'HSV-1',\n",
       " 'РИА',\n",
       " 'ინგლისურიდან',\n",
       " 'Münsterland',\n",
       " 'Newie',\n",
       " 'Lennard',\n",
       " 'Sproston',\n",
       " 'Doodleberrys',\n",
       " 'kidō',\n",
       " 'Tsuneyuki',\n",
       " 'ryulong',\n",
       " 'errors',\n",
       " 'Hafeez',\n",
       " '還有，那些slurs的來源我已經給你了，是你視而不見而已。客觀存在的東西為甚麼要無視呢？',\n",
       " 'non-domesticated',\n",
       " 'lehnen',\n",
       " 'Maurizio',\n",
       " '//en.wikipedia.org/wiki/Tania_De_Rozario',\n",
       " 'ഏജന്\\u200dസികളെ',\n",
       " 'OneClickJob.com',\n",
       " 'rewrite/delete',\n",
       " 'Bulgar',\n",
       " 'దాదాపు',\n",
       " '24035',\n",
       " 'OKF',\n",
       " 'Kutenai',\n",
       " 'prevailed',\n",
       " '/without',\n",
       " 'TII',\n",
       " 'poliert',\n",
       " '1ce',\n",
       " 'zivu',\n",
       " 'Kanungo',\n",
       " 'Pandemonium',\n",
       " 'EXCUSE',\n",
       " 'blahhhhhhhhhhhblahhhhhhhhhblahblahblah',\n",
       " 'Trilisser',\n",
       " 'magadhi',\n",
       " 'diffraction',\n",
       " 'gove',\n",
       " 'American-Iranian',\n",
       " \"'issue\",\n",
       " 'midziyo',\n",
       " 'creditied',\n",
       " 'Milne',\n",
       " 'ethno/history/culture/peoples',\n",
       " '108.45.75.145',\n",
       " '|attention=',\n",
       " 'rubbing',\n",
       " 'jizzing',\n",
       " 'planeta',\n",
       " 'ÖGB',\n",
       " 'sexualizing',\n",
       " 'uncoolest',\n",
       " 'hierarchy.',\n",
       " '२३७',\n",
       " 'special-',\n",
       " 'yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy',\n",
       " 'fife',\n",
       " '7:39.28',\n",
       " 'S3X',\n",
       " 'unreported',\n",
       " 'ef',\n",
       " 'havea',\n",
       " 'CRAWL',\n",
       " 'SR2',\n",
       " 'records.',\n",
       " 'ego-driven',\n",
       " 'sequiturs',\n",
       " 'जोबन',\n",
       " 'vip',\n",
       " '受',\n",
       " 'www.fclb.org/boards.htm',\n",
       " 'Meivazhi',\n",
       " 'BOBBY',\n",
       " 'nepeheline',\n",
       " 'bareknuckle',\n",
       " 'www.euclid.int',\n",
       " 'Lakshadweep',\n",
       " 'বিজ্ঞাপন',\n",
       " 'MODES',\n",
       " 'Bredin',\n",
       " 'textNOT',\n",
       " '190,000',\n",
       " 'field-dependent/field-independent',\n",
       " 'cuts-and-paste',\n",
       " 'faqqed',\n",
       " 'A8UDI',\n",
       " \"jofp'o\",\n",
       " ':Malkin',\n",
       " '*Dewey',\n",
       " 'Sunkiang/Songjiang',\n",
       " 'mangaka',\n",
       " '==Saint',\n",
       " 'unliklihood',\n",
       " 'Nominet',\n",
       " '`Former',\n",
       " 'SAFER',\n",
       " 'plovili',\n",
       " 'JEEZ',\n",
       " 'پوشش',\n",
       " 'Dorset',\n",
       " 'Maho',\n",
       " 'phaggot',\n",
       " 'contracts==',\n",
       " 'subalbugineal',\n",
       " 'noticesEAT',\n",
       " 'peaceniks',\n",
       " \"'association\",\n",
       " 'Armstrong-related',\n",
       " 'SOme',\n",
       " 'Rictor',\n",
       " 'unreachable',\n",
       " 'nslookup',\n",
       " '10242—10246',\n",
       " 'ASkmen.com',\n",
       " 'femte',\n",
       " 'irresistibly',\n",
       " 'souhnded',\n",
       " 'Grendlefuzz',\n",
       " 'Dorit',\n",
       " 'peein',\n",
       " 'branhces',\n",
       " '*George',\n",
       " 'vote==',\n",
       " 'UI',\n",
       " 'NOW-I',\n",
       " 'Khorasan..',\n",
       " 'Religiously',\n",
       " 'ROM.dsk',\n",
       " 'โครงการวิกิพีเดีย',\n",
       " 'Aspie-Quiz',\n",
       " 'येउन',\n",
       " 'Choudhury',\n",
       " 'me.v/r',\n",
       " '1-week',\n",
       " 'money-wasting',\n",
       " 'simialr',\n",
       " 'Releaser',\n",
       " 'Franco-canadien',\n",
       " 'FeloniousMonk',\n",
       " 'Fereidun',\n",
       " 'Dissertation',\n",
       " 'PurpleMesa',\n",
       " 'occipital',\n",
       " 'للشمال،',\n",
       " 'earful',\n",
       " 'instaed',\n",
       " 'EDITS-IS',\n",
       " 'SOAPboxing',\n",
       " 'Lyonnais',\n",
       " \"'nowhereland\",\n",
       " \"'similar\",\n",
       " '006',\n",
       " 'GAR',\n",
       " 'etnički',\n",
       " 'ELV',\n",
       " 'studentsKFOR',\n",
       " 'Salvatici',\n",
       " '12-11-2009.Rudd',\n",
       " 'Cayte',\n",
       " '*may*',\n",
       " 'Strunk',\n",
       " '*Mirani',\n",
       " 'mise-en-scene',\n",
       " 're-layout',\n",
       " 'Unsuspecting',\n",
       " '*********************************',\n",
       " '***RESTORED***',\n",
       " 'Dewald',\n",
       " '1,655/516',\n",
       " 'elicits',\n",
       " 'Dasavathaaram',\n",
       " 'Mississippians',\n",
       " 'p.54',\n",
       " 'Microorganism',\n",
       " 'Aloom',\n",
       " 'Basang',\n",
       " 'meaning.',\n",
       " 'history-merge',\n",
       " 'gezaaa',\n",
       " 'Huanois',\n",
       " 'ofense',\n",
       " '1758-1775',\n",
       " 'subject-verb',\n",
       " 'TPCM',\n",
       " 'bulbasaur',\n",
       " 'PageMaker',\n",
       " 'کوبوندن',\n",
       " 'Fuji-san',\n",
       " 'Vamp',\n",
       " 'Pahlawan',\n",
       " '19,300',\n",
       " 'Gallowglass',\n",
       " \"'free-to-air\",\n",
       " 'Pieterszoon',\n",
       " 'endlessly',\n",
       " 'conditonal',\n",
       " \"there'sanother\",\n",
       " ':ABSOLUTELY',\n",
       " '4.95',\n",
       " 'de-watchlisting',\n",
       " 'Farah',\n",
       " '|Sebastian',\n",
       " '94.1.254.207',\n",
       " \"'Presidential\",\n",
       " 'belief-systems',\n",
       " 'heritage.',\n",
       " 'rewprding',\n",
       " 'bagh',\n",
       " 'fire-cracker',\n",
       " 'snippy',\n",
       " 'READ',\n",
       " 'docked',\n",
       " 'DHS_LOGO.jpg',\n",
       " 'staffs',\n",
       " 'unaddressed',\n",
       " 'butterly',\n",
       " 'Quanify',\n",
       " 'Antebellum',\n",
       " 'We`re',\n",
       " 'Contributions/Hauke',\n",
       " 'swirled',\n",
       " 'Colborne',\n",
       " 'scally',\n",
       " 'Right-wingers',\n",
       " 'SESURE',\n",
       " '==GANs==',\n",
       " 'Taskforce',\n",
       " '*Bolaaq',\n",
       " 'OH-',\n",
       " ':yeah',\n",
       " 'Goffredo',\n",
       " 'ｎｏｔ',\n",
       " 'Shebang',\n",
       " 'high-profit',\n",
       " 'Bröckers',\n",
       " 'Memphis-Misraim',\n",
       " '188.96.228.73',\n",
       " 'Instant',\n",
       " 'India.',\n",
       " 'anti-colonialism',\n",
       " 'escapes',\n",
       " 'jw',\n",
       " 'ponyboy',\n",
       " 'applciation',\n",
       " 'António',\n",
       " 'Batcat',\n",
       " 'Duece-',\n",
       " 'Occonos',\n",
       " 'autographed',\n",
       " 'anti-blog',\n",
       " 'мислиш',\n",
       " '2-I',\n",
       " 'reounce',\n",
       " '02-9137824..',\n",
       " '71.191.24.206',\n",
       " 'jolicloud',\n",
       " 'Bbb23|Bbb23',\n",
       " 'Hurrian',\n",
       " 'insultingly',\n",
       " 'Sabaidee',\n",
       " 'endorsement-giver',\n",
       " 'collegeooo',\n",
       " 'FRYE',\n",
       " '==Merge',\n",
       " 'Wereith',\n",
       " 'kepercayaan',\n",
       " 'EPSL',\n",
       " '//en.wikipedia.org/wiki/North_Lancaster',\n",
       " '14,500',\n",
       " 'live.',\n",
       " 'fleurs',\n",
       " 'microinsurance',\n",
       " 'spricht',\n",
       " 'Tự',\n",
       " 'Nost',\n",
       " 'blir',\n",
       " '34.1',\n",
       " 'sport',\n",
       " 'Lager==',\n",
       " 'minstrel',\n",
       " '2012—if',\n",
       " '//green.blogs.nytimes.com/2012/11/20/c-i-a-closes-its-climate-change-office/',\n",
       " '==Neither',\n",
       " 'الذىين',\n",
       " 'testwiki',\n",
       " 'I*',\n",
       " '82.132.214.222',\n",
       " 'לצחוק',\n",
       " \"'petty-mindedness\",\n",
       " 'Bajaj',\n",
       " 'convulsives',\n",
       " 'broaden',\n",
       " 'Kendra',\n",
       " 'meztiso',\n",
       " 'peerages',\n",
       " 'DF67',\n",
       " 'Daquq',\n",
       " 'oomph',\n",
       " 'مَا',\n",
       " 'Claudian',\n",
       " 'Fire-beacon',\n",
       " 'megnõtt',\n",
       " 'Aquamax',\n",
       " 'Katsuya',\n",
       " ':Hate',\n",
       " 'Shahrukh',\n",
       " 'muons',\n",
       " '1882–1966',\n",
       " 'saws',\n",
       " 'multitasking',\n",
       " 'Yerevanci',\n",
       " 'Losses-destroyed',\n",
       " 'lagoons',\n",
       " 'вас',\n",
       " 'CCV-564',\n",
       " 'compensates',\n",
       " 'speed=',\n",
       " 'perspective',\n",
       " 'Qizard',\n",
       " 'Franca',\n",
       " 'non-Tsunamis',\n",
       " 'Jèrriais',\n",
       " 'determinded',\n",
       " '2003-04',\n",
       " 'wayward',\n",
       " 'Ausley',\n",
       " 'eReview',\n",
       " 'okruženja',\n",
       " 'IDEALOGICAL',\n",
       " 'tl|example',\n",
       " 'crash..',\n",
       " 'Specialists',\n",
       " 'fukcng',\n",
       " 'Almada',\n",
       " 'Biblesource',\n",
       " 'ﻷﺣﻀﺮ200',\n",
       " 'auhor',\n",
       " 'www.vit.edu',\n",
       " 'Gjuhes',\n",
       " 'Supe',\n",
       " 'e.g.my',\n",
       " 'ruthlessly',\n",
       " 'misinterpreted.',\n",
       " 'F2ComButton',\n",
       " 'Ottar',\n",
       " 'fall/autumn',\n",
       " 'region-references',\n",
       " 'Citgo',\n",
       " 'chinchillas',\n",
       " 'al-Qawuqji',\n",
       " 'AL-Mansoury',\n",
       " 'الطرفية',\n",
       " 'Evading',\n",
       " 'samaj',\n",
       " '89.233.252.43',\n",
       " '͉̳̯̀̌T͉̼̈́̆͒ͨ̏̌R͗̎̏͒ͬ̊͐̋͏̳͚̟͎̭͘͟I̫͎̪͊͊͑͐ͬ̎͞X̬̘̖͍͆̉̆̒̋̀I̪̻͒̎ͦ̈́̎͂̀̍͞͞Ẹ̴̖̝̗̳̪ͣ͛̔̽',\n",
       " 'αγωνιστεί',\n",
       " '71.120.128.212',\n",
       " 'jesuit',\n",
       " 'miclicous',\n",
       " '15:41',\n",
       " '7,63',\n",
       " 'GeoIP',\n",
       " 'Serbian/Croatian',\n",
       " '==Esata==',\n",
       " 'ewish',\n",
       " 're-rolling',\n",
       " 'Boulder',\n",
       " '一丈青',\n",
       " 'G-Clef',\n",
       " 'expend',\n",
       " 'Medic',\n",
       " 'Liberal/Leftist',\n",
       " 'postei',\n",
       " 'state-executed',\n",
       " 'hester',\n",
       " 'scripts.May',\n",
       " 'boild',\n",
       " 'banaadir',\n",
       " 'Martello',\n",
       " 'youfuck',\n",
       " 'NaaTapati',\n",
       " 'handeln',\n",
       " '==Tag==',\n",
       " 'princess-ship',\n",
       " ...}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(token_set) == toxic.vocab_size\n",
    "assert toxic.token_set - token_set == set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Vocab Dictionary\n",
    "\n",
    "In training we want to take words and then lookup our vectors using an index. In this step we want to assign unique indices to each token, and define a way to lookup indices given tokens. We will use a `dict` for this.\n",
    "\n",
    "At the end of this step you should have a dictionary variable called `vocab`. Run the following cell to confirm this is all in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dict(zip(token_set, range(len(token_set))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vocab) == toxic.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the Training Pairs\n",
    "\n",
    "Before coding deep learning models it is necessary to first fully think through how we are going to present the data to the network. We have an idea of this from the slides/notes. It is still a good idea to put this step here in the workflow, before coding up the model. This will avoid having to make annoying changes that might follow from small details that are easy to overlook.\n",
    "\n",
    "We know we are going to present two words at a time: a center word, and a context word. But how are we going to present them: as tokens, or as indices? These details matter when you code the forward pass of the network. If you try an embedding lookup with a string, you will see an error. We will use integer indices as it will be every so slightly faster than adding a dictionary lookup as well at training time.\n",
    "\n",
    "Since implementing finding the context tokens for all words over all instances in the dataset is not generally useful, we do that for you. In practice, you can just use gensim (https://radimrehurek.com/gensim/) to implement word2vec models. This is a learning exercise. Only if you really want to start drastically customizing your model would you really use your own implementaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5  # our context window size - play with different sizes to get a better feel for how this works\n",
    "contexts = toxic.contexts(train, test, vocab, m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contexts` variable is a dictionary where the keys are the indices of all the tokens in the dataset, and the values are `set`s of token indices that occur in their contexts. We will sample from these during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement Negative Sampling\n",
    "\n",
    "We need to define a function that takes a token (or its index) and returns a list of negative samples, sampled by our function based on the token frequency distribution. This function will therefore depend on the state of the vocabulary and token frequencies (the data structure we just built). For this reason we will define a class that wraps this information and allows us to call it with new word indices for every mini-batch in training. We will implement it as a callable in python (by overloading the `__call__` method). This can be syntactically nicer than defining a function on the class because it is simpler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    \n",
    "    def __init__(self, vocab, frequencies, contexts):\n",
    "        \"\"\"Create a new NegativeSampler.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary.\n",
    "          frequencies: List of integers, the frequencies of each word,\n",
    "            sorted in word index order.\n",
    "          contexts: Dictionary.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)\n",
    "        self.contexts = contexts\n",
    "        self.distribution = self.p(frequencies)\n",
    "    \n",
    "    def __call__(self, tok_ix, num_negs):\n",
    "        \"\"\"Get negative samples.\n",
    "        \n",
    "        Args:\n",
    "          tok_ix: Integer, the index of the center word.\n",
    "          num_negs: Integer, the number of negative samples to take.\n",
    "        \"\"\"\n",
    "        # don't sample the center word or its context\n",
    "        distribution = self.distribution.copy()\n",
    "        distribution[tok_ix] = 0.\n",
    "        distribution[self.contexts[tok_ix]] = 0.\n",
    "        return np.random.choice(range(self.n), size=num_negs, p=distribution)\n",
    "    \n",
    "    def p(self, frequencies):\n",
    "        \"\"\"Determine the probability distribution for negative sampling.\n",
    "        \n",
    "        Args:\n",
    "          frequencies: List of integers.\n",
    "        \n",
    "        Returns:\n",
    "          numpy.array.\n",
    "        \"\"\"\n",
    "        frequences = np.array(frequencies)\n",
    "        return np.power(frequencies, 3/4) / np.sum(np.power(frequencies, 3/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Code the Model\n",
    "\n",
    "Now for the fun part.\n",
    "\n",
    "Notice that for all the fancy neural network diagrams you see in word2vec tutorials, when you get down to the maths it is just picking vectors out of matrices and calculating their dot products? It's nothing to be frightened of. Just follow the equations closely and it will work. We don't even need to use any complicated functions or neural network modules. In fact, we could do it in numpy (but then we would have to calculate the gradients manually). PyTorch makes this very easy.\n",
    "\n",
    "Below is a template for the model. You need to implement the functions that say `### Impelement Me ###`. They should be self explanatory from the slides/notes. Refer to them if stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"SkipGram Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, emb_dim, num_negs, lr):\n",
    "        \"\"\"Create a new SkipGram.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary, our vocab dict with token keys and index values.\n",
    "          emb_dim: Integer, the size of word embeddings.\n",
    "          num_negs: Integer, the number of non-context words to sample.\n",
    "          lr: Float, the learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)  # size of the vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        ### Implement Me: Initialize self.U and self.V,      ###\n",
    "        ### the parameter matrices of the network.           ###\n",
    "        ### They need to be nn.Parameter objects.            ###\n",
    "        ### V should have word vectors as row matrices,      ###\n",
    "        ### U should have word vectors as column matrices.   ###\n",
    "        ### Use nn.init.uniform() to perform random uniform  ###\n",
    "        ### initialization in the range [-0.01, 0.01].       ###\n",
    "            \n",
    "        self.U = nn.Parameter(torch.Tensor(n, emb_dim), requires_grad=True)\n",
    "        self.V = nn.Parameter(torch.Tensor(emb_dim, n), requires_grad=True)\n",
    "        nn.init.uniform(self.V.weight, a=-0.01, b=0.01)\n",
    "        nn.init.uniform(self.U.weight, a=-0.01, b=0.01)        \n",
    "        \n",
    "        ### End Implement ##\n",
    "        \n",
    "        # Adam is a good optimizer and will converge faster than SGD\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)  \n",
    "    \n",
    "    def forward(self, center_ixs, context_ixs):\n",
    "        \"\"\"Compute the forward pass of the network.\n",
    "        \n",
    "        1. Lookup embeddings for center and context words\n",
    "            - use self.lookup()\n",
    "        2. Sample negative samples\n",
    "            - use self.negative_samples()\n",
    "        2. Calculate the probability estimates\n",
    "            - make sure to implement and use self.softmax()\n",
    "        3. Calculate the loss\n",
    "            - using self.loss()\n",
    "        \n",
    "        Args:\n",
    "          center_ixs: List of integer indices.\n",
    "          context_ixs: List of integer indices.\n",
    "        \n",
    "        Returns:\n",
    "          loss (torch.autograd.Variable).\n",
    "        \"\"\"\n",
    "        ### Implement Me ###\n",
    "        center_words = self.embedding_lookup(self.V, center_ixs)\n",
    "        context_words = self.embedding_lookup(self.U, context_ixs)\n",
    "        negatives = self.negative_samples(center_words)\n",
    "        context_plus_negs = torch.stack([context_words, negatives])\n",
    "        logits = center_words.matmul(context_plus_negs)\n",
    "        preds = self.softmax(logits)\n",
    "        targets = torch.LongTensor(context_ixs)\n",
    "        return self.loss(preds, targets)\n",
    "    \n",
    "    def lookup(embedding, indices):\n",
    "        \"\"\"Lookup embeddings given indices.\n",
    "        \n",
    "        Args:\n",
    "          embedding: nn.Parameter, an embedding matrix.\n",
    "          indices: List of integers, the indices to lookup.\n",
    "        \n",
    "        Returns:\n",
    "          torch.autograd.Variable of shape [len(indices), emb_dim]. A matrix \n",
    "            with horizontally stacked word vectors.\n",
    "        \"\"\"\n",
    "        lookup_tensor = torch.LongTensor(indices)\n",
    "        return embedding[lookup_tensor]\n",
    "    \n",
    "    def loss(self, preds, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\n",
    "        \n",
    "        Implement this for practice, don't use the built-in PyTorch function.\n",
    "        \n",
    "        Args:\n",
    "          preds: Tensor of shape [batch_size, vocab_size], our predictions.\n",
    "          targets: List of integers, the vocab indices of target context words.\n",
    "        \"\"\"\n",
    "        ### Implement Me ###\n",
    "        return -1 * torch.sum(targets * torch.log(preds))\n",
    "    \n",
    "    def optimize(self, loss):\n",
    "        \"\"\"Optimization step.\n",
    "        \n",
    "        Args:\n",
    "          loss: Scalar.\n",
    "        \"\"\"\n",
    "        # We first make sure to remove any previous gradient from our tensors\n",
    "        # before calculating again.\n",
    "        self.optimizer.zero_grad()\n",
    "        ### Implement Me ###\n",
    "        loss.backward()\n",
    "        self.optimizer.step()        \n",
    "    \n",
    "    def softmax(self, logits):\n",
    "        \"\"\"Compute the softmax function.\n",
    "        \n",
    "        Implement this for practice, don't use the built-in PyTorch function.\n",
    "        \n",
    "        Args:\n",
    "          logits: Tensor of shape [batch_size, vocab_size].\n",
    "        \n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, vocab_size], our predictions.\n",
    "        \"\"\"\n",
    "        ### Impelement Me ###\n",
    "        return torch.exp(logits) / torch.sum(torch.exp(logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "-4.2402e+09  4.5838e-41 -4.3134e-19\n",
      " 3.0649e-41  5.0217e+03  4.5838e-41\n",
      " 5.0217e+03  4.5838e-41  5.0217e+03\n",
      " 4.5838e-41  5.0217e+03  4.5838e-41\n",
      "[torch.FloatTensor of size 4x3]\n",
      "\n",
      "Variable containing:\n",
      "-4.2402e+09  4.5838e-41 -4.3134e-19\n",
      " 5.0217e+03  4.5838e-41  5.0217e+03\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
