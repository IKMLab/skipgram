{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zake7749/.local/lib/python3.5/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, SpatialDropout1D, Reshape, Conv2D, MaxPooling2D, merge, Flatten, Bidirectional, CuDNNGRU, add, Conv1D, MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = 'Dataset/'\n",
    "#EMBEDDING_FILE='features/fast-text-300.txt'\n",
    "EMBEDDING_FILE='features/glove.840B.300d.txt'\n",
    "TRAIN_DATA_FILE=path + 'train.csv'\n",
    "TEST_DATA_FILE=path + 'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 700\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Err on  ['.', '.', '.']\n",
      "Err on  ['at', 'name@domain.com', '0.0061218']\n",
      "Err on  ['.', '.', '.']\n",
      "Err on  ['to', 'name@domain.com', '0.33865']\n",
      "Err on  ['.', '.', '0.035974']\n",
      "Err on  ['.', '.', '.']\n",
      "Err on  ['email', 'name@domain.com', '0.33529']\n",
      "Err on  ['or', 'name@domain.com', '0.48374']\n",
      "Err on  ['contact', 'name@domain.com', '0.016426']\n",
      "Err on  ['Email', 'name@domain.com', '0.37344']\n",
      "Err on  ['on', 'name@domain.com', '0.037295']\n",
      "Err on  ['At', 'Killerseats.com', '-0.13854']\n",
      "Err on  ['by', 'name@domain.com', '0.6882']\n",
      "Err on  ['in', 'mylot.com', '-0.18148']\n",
      "Err on  ['emailing', 'name@domain.com', '0.39173']\n",
      "Err on  ['Contact', 'name@domain.com', '0.14933']\n",
      "Err on  ['at', 'name@domain.com', '0.44321']\n",
      "Err on  ['•', 'name@domain.com', '-0.13288']\n",
      "Err on  ['at', 'Amazon.com', '-0.5275']\n",
      "Err on  ['is', 'name@domain.com', '-0.1197']\n",
      "Total 2195884 word vectors.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "#Glove Vectors\n",
    "embeddings_index = {}\n",
    "f = open(EMBEDDING_FILE, 'r', encoding='utf-8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    try:\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(\"Err on \", values[:3])\n",
    "f.close()\n",
    "\n",
    "print('Total %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "# Load the cleaned words\n",
    "########################################\n",
    "\n",
    "cl_path = 'features/cleanwords.txt'\n",
    "clean_word_dict = {}\n",
    "with open(cl_path, 'r', encoding='utf-8') as cl:\n",
    "    for line in cl:\n",
    "        line = line.strip('\\n')\n",
    "        typo, correct = line.split(',')\n",
    "        clean_word_dict[typo] = correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n",
      "Found 332483 unique tokens\n",
      "Shape of data tensor: (159571, 700)\n",
      "Shape of label tensor: (159571, 6)\n",
      "Shape of test_data tensor: (153164, 700)\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "import re\n",
    "\n",
    "print('Processing text dataset')\n",
    "from collections import defaultdict\n",
    "# Regex to remove all Non-Alpha Numeric and space\n",
    "special_character_removal=re.compile(r'[^a-z\\d ]',re.IGNORECASE)\n",
    "\n",
    "# regex to replace all numerics\n",
    "replace_numbers=re.compile(r'\\d+',re.IGNORECASE)\n",
    "word_count_dict = defaultdict(int)\n",
    "toxic_dict = {}\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False, count_null_words=True, clean_wiki_tokens=True):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    # dirty words\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{2,256}\\.[a-z]{2,6}\\b([-a-zA-Z0-9@:%_\\+.~#?&//=]*)\", \"\", text)\n",
    "    text = re.sub(r\"(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}\", \"\", text)\n",
    "    \n",
    "    if clean_wiki_tokens:\n",
    "        # Clean the image\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*.jpg\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*.png\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*.gif\", \" \", text)\n",
    "        text = re.sub(r\"image:[a-zA-Z0-9]*.bmp\", \" \", text)\n",
    "\n",
    "        # Special wiki token\n",
    "        text = re.sub(r\"{{[a-zA-Z0-9]*}}\", \" \", text)\n",
    "        text = re.sub(r'\"{2,}', \" \", text)\n",
    "        text = re.sub(r'={2,}', \" \", text)\n",
    "        text = re.sub(r':{2,}', \" \", text)\n",
    "        \n",
    "        #text = re.sub(, \"\", text)\n",
    "    \n",
    "    for typo, correct in clean_word_dict.items():\n",
    "        text = re.sub(typo, \" \" + correct + \" \", text)\n",
    "\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"i’m\", \"i am\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = replace_numbers.sub('', text)\n",
    "    #text = special_character_removal.sub('',text)\n",
    "\n",
    "    if count_null_words:\n",
    "        text = text.split()\n",
    "        for t in text:\n",
    "            word_count_dict[t] += 1\n",
    "        text = \" \".join(text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "\n",
    "    return (text)\n",
    "\n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[list_classes].values\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"no comment\").values\n",
    "\n",
    "comments = []\n",
    "for text in list_sentences_train:\n",
    "    comments.append(text_to_wordlist(text))\n",
    "    \n",
    "test_comments=[]\n",
    "for text in list_sentences_test:\n",
    "    test_comments.append(text_to_wordlist(text))\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(comments + test_comments)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_comments)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', data.shape)\n",
    "print('Shape of label tensor:', y.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of test_data tensor:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 24784\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "\n",
    "null_words = open('null-word.txt', 'w', encoding='utf-8')\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) +'\\n')\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        null_words.write(word + ', ' + str(word_count_dict[word]) + '\\n')\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "\n",
    "#24146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yo bitch ja rule is more succesful then you will ever be whats up with you and hating you sad mofuckas i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me ja rule is about pride in da music man dont diss that shit on him and nothin is wrong bein like tupac he was a brother too fuckin white boys get things right next time',\n",
       " 'from rfc the title is fine as it is imo',\n",
       " '\" sources * zawe ashton on lapland — \"',\n",
       " ': if you have a look back at the source the information i updated was the correct form i can only guess the source had not updated i shall update the information once again but thank you for your message',\n",
       " 'i do not anonymously edit articles at all',\n",
       " 'thank you for understanding i think very highly of you and would not revert without discussion',\n",
       " 'please do not add nonsense to wikipedia such edits are considered vandalism and quickly undone if you would like to experiment please use the sandbox instead thank you -',\n",
       " ': dear god this site is horrible',\n",
       " '\" only a fool can believe in such numbers the correct number lies between to ponder the numbers carefully this error will persist for a long time as it continues to reproduce the latest reproduction i know is from encyclopædia britannica almanac wich states magnittude : (fair enough) victims : (today to is not a lot so i guess people just come out with a number that impresses enough i do not know but i know this : it just a shameless lucky number that they throw in the air gc \"',\n",
       " 'double redirects when fixing double redirects do not just blank the outer one you need edit it to point it to the final target unless you think it inappropriate in which case it needs to be nominated at wp : rfd',\n",
       " 'i think its crap that the link to roggenbier is to this article somebody that knows how to do things should change it',\n",
       " '\" somebody will invariably try to add religion? really?? you mean the way people have invariably kept adding religion to the samuel beckett infobox? and why do you bother bringing up the long - dead completely non - existent influences issue? you are just flailing making up crap on the fly for comparison the only explicit acknowledgement in the entire amos oz article that he is personally jewish is in the categories ! \"',\n",
       " 'february (utc) looking it over it clear that (a banned sockpuppet of ) ignored the consensus (& fwiw policy - appropriate) choice to leave the page at chihuahua (mexico) and the current page should be returned there anyone have the time to fix the incoming links? - :',\n",
       " '\" it says it right there that it is a type the type of institution is needed in this case because there are three levels of suny schools : - university centers and doctoral granting institutions - state colleges - community colleges it is needed in this case to clarify that ub is a suny center it says it even in binghamton university university at albany state university of new york and stony brook university stop trying to say it not because i am totally right in this case \"',\n",
       " '\" before adding a new product to the list make sure it relevant before adding a new product to the list make sure it has a wikipedia entry already proving it relevance and giving the reader the possibility to read more about it otherwise it could be subject to deletion see this article revision history \"',\n",
       " 'current position anyone have confirmation that sir alfred is no longer at the airport and is hospitalised?',\n",
       " 'this other one from',\n",
       " 'reason for banning throwing this article needs a section on why throwing is banned at the moment to a non - cricket fan it seems kind of arbitrary',\n",
       " 'wallamoose was changing the cited material to say things the original source did not say in response to his objections i modified the article as we went along i was not just reverting him i repeatedly asked him to use the talk page i have been trying to add to the article for a long time it so thin on content this is wrong',\n",
       " '|blocked]] from editing wikipedia |']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comments[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sort null word\n",
    "null_count = {}\n",
    "with open('null-word.txt', 'r', encoding='utf-8') as nullword:\n",
    "    for line in nullword:\n",
    "        w, c = line.strip('\\n').split(', ')\n",
    "        null_count[w] = int(c)\n",
    "null_count = sorted(null_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "with open('null-word.txt', 'w', encoding='utf-8') as output:\n",
    "    for w, c in null_count:\n",
    "        output.write(w + \", \" + str(c) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Dense, Activation, Multiply, Add, Lambda\n",
    "import keras.initializers\n",
    " \n",
    "def highway_layers(value, n_layers, activation=\"tanh\", gate_bias=-3):\n",
    "    dim = K.int_shape(value)[-1]\n",
    "    gate_bias_initializer = keras.initializers.Constant(gate_bias)\n",
    "    for i in range(n_layers):     \n",
    "        gate = Dense(units=dim, bias_initializer=gate_bias_initializer)(value)\n",
    "        gate = Activation(\"sigmoid\")(gate)\n",
    "        negated_gate = Lambda(\n",
    "            lambda x: 1.0 - x,\n",
    "            output_shape=(dim,))(gate)\n",
    "        transformed = Dense(units=dim)(value)\n",
    "        transformed = Activation(activation)(value)\n",
    "        transformed_gated = Multiply()([gate, transformed])\n",
    "        identity_gated = Multiply()([negated_gate, value])\n",
    "        value = Add()([transformed_gated, identity_gated])\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the RNN with Attention model structure\n",
    "########################################\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers import AveragePooling1D, GlobalMaxPooling1D, GlobalAveragePooling1D, Lambda\n",
    "from keras import optimizers\n",
    "adam_optimizer = optimizers.Adam(lr=1e-3, clipvalue=1, clipnorm=1)\n",
    "\n",
    "def get_dropout_bi_gru():\n",
    "    recurrent_units = 48\n",
    "    dropout_rate = 0.35\n",
    "    dense_size = 32\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)(input_layer)\n",
    "    embedding_layer = SpatialDropout1D(0.15)(embedding_layer)\n",
    "    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedding_layer)\n",
    "    x = Dropout(0.35)(x)\n",
    "    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(x)\n",
    "    \n",
    "    last = Lambda(lambda t: t[:, -1])(x)\n",
    "    maxpool = GlobalMaxPooling1D()(x)\n",
    "    average = GlobalAveragePooling1D()(x)\n",
    "    concatenated = concatenate([last, maxpool, average], axis=1)\n",
    "    x = Dropout(0.5)(maxpool)\n",
    "    x = Dense(72, activation=\"relu\")(x)\n",
    "    output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_optimizer,\n",
    "    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division\n",
    "\n",
    "import sys\n",
    "from os.path import dirname\n",
    "from keras import initializers\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "class AttentionWeightedAverage(Layer):\n",
    "    \"\"\"\n",
    "    Computes a weighted average of the different channels across timesteps.\n",
    "    Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, return_attention=False, **kwargs):\n",
    "        self.init = initializers.get('uniform')\n",
    "        self.supports_masking = True\n",
    "        self.return_attention = return_attention\n",
    "        super(AttentionWeightedAverage, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.input_spec = [InputSpec(ndim=3)]\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[2], 1),\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 initializer=self.init)\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttentionWeightedAverage, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        # computes a probability distribution over the timesteps\n",
    "        # uses 'max trick' for numerical stability\n",
    "        # reshape is done to avoid issue with Tensorflow\n",
    "        # and 1-dimensional weights\n",
    "        logits = K.dot(x, self.W)\n",
    "        x_shape = K.shape(x)\n",
    "        logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "        ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "\n",
    "        # masked timesteps have zero weight\n",
    "        if mask is not None:\n",
    "            mask = K.cast(mask, K.floatx())\n",
    "            ai = ai * mask\n",
    "        att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "        weighted_input = x * K.expand_dims(att_weights)\n",
    "        result = K.sum(weighted_input, axis=1)\n",
    "        if self.return_attention:\n",
    "            return [result, att_weights]\n",
    "        return result\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return self.compute_output_shape(input_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_len = input_shape[2]\n",
    "        if self.return_attention:\n",
    "            return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n",
    "        return (input_shape[0], output_len)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        if isinstance(input_mask, list):\n",
    "            return [None] * len(input_mask)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the RNN with Attention model structure\n",
    "########################################\n",
    "\n",
    "from keras import optimizers\n",
    "adam_optimizer = optimizers.Adam(lr=1e-3, clipvalue=1, clipnorm=1, decay=1e-10)\n",
    "\n",
    "def get_plain_attention_rnn():\n",
    "    recurrent_units = 72\n",
    "    dropout_rate = 0.35\n",
    "    dense_size = 32\n",
    "    input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)(input_layer)\n",
    "    embedding_layer = SpatialDropout1D(0.25)(embedding_layer)\n",
    "    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(embedding_layer)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Bidirectional(CuDNNGRU(recurrent_units, return_sequences=True))(x)\n",
    "    \n",
    "    last = Lambda(lambda t: t[:, -1])(x)\n",
    "    maxpool = GlobalMaxPooling1D()(x)\n",
    "    attn = AttentionWeightedAverage()(x)\n",
    "    average = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    concatenated = concatenate([attn, maxpool, last, average], axis=1)\n",
    "    x = Dropout(0.62)(concatenated)\n",
    "    x = Dense(72, activation=\"relu\")(x)\n",
    "    output_layer = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=adam_optimizer,\n",
    "    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import Flatten\n",
    "import tensorflow as tf\n",
    "\n",
    "class KMaxPooling(Layer):\n",
    "    \"\"\"\n",
    "    K-max pooling layer that extracts the k-highest activations from a sequence (2nd dimension).\n",
    "    TensorFlow backend.\n",
    "    \"\"\"\n",
    "    def __init__(self, k=1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_spec = InputSpec(ndim=3)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], (input_shape[2] * self.k))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        # swap last two dimensions since top_k will be applied along the last dimension\n",
    "        shifted_input = tf.transpose(inputs, [0, 2, 1])\n",
    "        \n",
    "        # extract top_k, returns two tensors [values, indices]\n",
    "        top_k = tf.nn.top_k(shifted_input, k=self.k, sorted=True, name=None)[0]\n",
    "        \n",
    "        # return flattened output\n",
    "        return Flatten()(top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the RNN with Attention model structure\n",
    "########################################\n",
    "\n",
    "from keras import optimizers\n",
    "adam_optimizer = optimizers.Adam(lr=1e-3)\n",
    "\n",
    "def get_kmax_text_cnn():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "\n",
    "    filter_nums = 120\n",
    "    drop = 0.5\n",
    "    \n",
    "    comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences= embedding_layer(comment_input)\n",
    "    embedded_sequences = SpatialDropout1D(0.2)(embedded_sequences)\n",
    "    \n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "\n",
    "    #conv_0 = Conv1D(filter_nums / 2, 1, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(conv_0)\n",
    "    #conv_1 = Conv1D(filter_nums / 2, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(conv_1)\n",
    "    #conv_2 = Conv1D(filter_nums / 2, 3, strides=2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(conv_2)\n",
    "\n",
    "    maxpool_0 = KMaxPooling(k=4)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=4)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=4)(conv_2)\n",
    "\n",
    "    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2], mode='concat', concat_axis=1)\n",
    "    output = Dropout(drop)(merged_tensor)\n",
    "    output = Dense(units=120, activation=\"relu\")(output)\n",
    "    output = Dropout(0.3)(output)\n",
    "    output = Dense(units=6, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model(inputs=comment_input, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the RNN with Attention model structure\n",
    "########################################\n",
    "\n",
    "from keras import optimizers\n",
    "adam_optimizer = optimizers.Adam(lr=1e-3)\n",
    "\n",
    "def get_higway_cnn():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH,\n",
    "            trainable=False)\n",
    "\n",
    "    filter_nums = 120\n",
    "    drop = 0.5\n",
    "    \n",
    "    comment_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "    embedded_sequences= embedding_layer(comment_input)\n",
    "\n",
    "    conv_0 = Conv1D(filter_nums, 1, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "    conv_1 = Conv1D(filter_nums, 2, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "    conv_2 = Conv1D(filter_nums, 3, kernel_initializer=\"normal\", padding=\"valid\", activation=\"relu\")(embedded_sequences)\n",
    "\n",
    "    maxpool_0 = KMaxPooling(k=4)(conv_0)\n",
    "    maxpool_1 = KMaxPooling(k=4)(conv_1)\n",
    "    maxpool_2 = KMaxPooling(k=4)(conv_2)\n",
    "\n",
    "    merged_tensor = merge([maxpool_0, maxpool_1, maxpool_2], mode='concat', concat_axis=1)\n",
    "    output = Dropout(drop)(merged_tensor)\n",
    "    output = Dense(units=96)(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Activation('relu')(output)\n",
    "    output = highway_layers(output, 3)\n",
    "    output = Dense(units=6, activation='sigmoid')(output)\n",
    "\n",
    "    model = Model(inputs=comment_input, outputs=output)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Reshape, Permute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Fold Cross Valiadtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "STAMP = 'pavel_rnn_%.2f_%.2f'%(0.5,0.5)\n",
    "\n",
    "def _train_model_by_auc(model, batch_size, train_x, train_y, val_x, val_y):\n",
    "    best_auc = -1\n",
    "    best_weights = None\n",
    "    best_epoch = 0\n",
    "\n",
    "    current_epoch = 1\n",
    "\n",
    "    while True:\n",
    "        model.fit(train_x, train_y, batch_size=batch_size, epochs=1, validation_data=[val_x, val_y])\n",
    "        y_pred = model.predict(val_x, batch_size=batch_size)\n",
    "        current_auc = roc_auc_score(val_y, y_pred)\n",
    "        print(\"Epoch {} auc {:.6f} best_auc {:.6f}\".format(current_epoch, current_auc, best_auc))\n",
    "        current_epoch += 1\n",
    "        if best_auc < current_auc or best_auc == -1:\n",
    "            best_auc = current_auc\n",
    "            best_weights = model.get_weights()\n",
    "            best_epoch = current_epoch\n",
    "        else:\n",
    "            if current_epoch - best_epoch == 5:\n",
    "                break\n",
    "\n",
    "    model.set_weights(best_weights)\n",
    "    return model, best_auc\n",
    "\n",
    "def _train_model_by_logloss(model, batch_size, train_x, train_y, val_x, val_y, fold_id):\n",
    "    early_stopping =EarlyStopping(monitor='val_loss', patience=6)\n",
    "    bst_model_path = STAMP + str(fold_id) + '.h5'\n",
    "    model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "    hist = model.fit(train_x, train_y,\n",
    "        validation_data=(val_x, val_y),\n",
    "        epochs=50, batch_size=batch_size, shuffle=True,\n",
    "        callbacks=[early_stopping, model_checkpoint])\n",
    "    bst_val_score = min(hist.history['val_loss'])\n",
    "    predictions = model.predict(val_x)\n",
    "    auc = roc_auc_score(val_y, predictions)\n",
    "    return model, bst_val_score, auc, predictions\n",
    "\n",
    "def train_folds(X, y, fold_count, batch_size, get_model_func):\n",
    "    fold_size = len(X) // fold_count\n",
    "    models = []\n",
    "    fold_predictions = []\n",
    "    score = 0\n",
    "    total_auc = 0\n",
    "    for fold_id in range(0, fold_count):\n",
    "        fold_start = fold_size * fold_id\n",
    "        fold_end = fold_start + fold_size\n",
    "\n",
    "        if fold_id == fold_size - 1:\n",
    "            fold_end = len(X)\n",
    "\n",
    "        train_x = np.concatenate([X[:fold_start], X[fold_end:]])\n",
    "        train_y = np.concatenate([y[:fold_start], y[fold_end:]])\n",
    "\n",
    "        val_x = X[fold_start:fold_end]\n",
    "        val_y = y[fold_start:fold_end]\n",
    "    \n",
    "        print(\"In fold #\", fold_id)\n",
    "        model, bst_val_score, auc, fold_prediction = _train_model_by_logloss(get_model_func(), batch_size, train_x, train_y, val_x, val_y, fold_id)\n",
    "        score += bst_val_score\n",
    "        total_auc += auc\n",
    "        fold_predictions.append(fold_prediction)\n",
    "        models.append(model)\n",
    "    return models, score / fold_count, total_auc / fold_count, fold_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In fold # 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0689 - acc: 0.9757 - val_loss: 0.0460 - val_acc: 0.9820\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0467 - acc: 0.9823 - val_loss: 0.0418 - val_acc: 0.9832\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0437 - acc: 0.9832 - val_loss: 0.0401 - val_acc: 0.9838\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0414 - acc: 0.9836 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0402 - acc: 0.9841 - val_loss: 0.0396 - val_acc: 0.9839\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0387 - acc: 0.9845 - val_loss: 0.0384 - val_acc: 0.9846\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0378 - acc: 0.9848 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0366 - acc: 0.9853 - val_loss: 0.0391 - val_acc: 0.9842\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0359 - acc: 0.9856 - val_loss: 0.0398 - val_acc: 0.9840\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0349 - acc: 0.9859 - val_loss: 0.0398 - val_acc: 0.9844\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0340 - acc: 0.9862 - val_loss: 0.0403 - val_acc: 0.9847\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0332 - acc: 0.9865 - val_loss: 0.0399 - val_acc: 0.9843\n",
      "In fold # 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 223s 2ms/step - loss: 0.0565 - acc: 0.9796 - val_loss: 0.0468 - val_acc: 0.9817\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0442 - acc: 0.9828 - val_loss: 0.0430 - val_acc: 0.9827\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0414 - acc: 0.9837 - val_loss: 0.0414 - val_acc: 0.9834\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0398 - acc: 0.9842 - val_loss: 0.0416 - val_acc: 0.9836\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0385 - acc: 0.9846 - val_loss: 0.0417 - val_acc: 0.9830\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0373 - acc: 0.9850 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0362 - acc: 0.9854 - val_loss: 0.0394 - val_acc: 0.9842\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0352 - acc: 0.9857 - val_loss: 0.0396 - val_acc: 0.9840\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0399 - val_acc: 0.9838\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0337 - acc: 0.9862 - val_loss: 0.0400 - val_acc: 0.9840\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0329 - acc: 0.9865 - val_loss: 0.0413 - val_acc: 0.9829\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0321 - acc: 0.9869 - val_loss: 0.0419 - val_acc: 0.9826\n",
      "Epoch 13/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0313 - acc: 0.9871 - val_loss: 0.0409 - val_acc: 0.9836\n",
      "In fold # 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0564 - acc: 0.9795 - val_loss: 0.0438 - val_acc: 0.9832\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0443 - acc: 0.9830 - val_loss: 0.0416 - val_acc: 0.9838\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0415 - acc: 0.9838 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0396 - acc: 0.9842 - val_loss: 0.0414 - val_acc: 0.9832\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0385 - acc: 0.9846 - val_loss: 0.0393 - val_acc: 0.9846\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0373 - acc: 0.9851 - val_loss: 0.0401 - val_acc: 0.9838\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0363 - acc: 0.9854 - val_loss: 0.0396 - val_acc: 0.9845\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0355 - acc: 0.9856 - val_loss: 0.0397 - val_acc: 0.9843\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0401 - val_acc: 0.9841\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0335 - acc: 0.9863 - val_loss: 0.0409 - val_acc: 0.9845\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0328 - acc: 0.9866 - val_loss: 0.0402 - val_acc: 0.9840\n",
      "In fold # 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0568 - acc: 0.9795 - val_loss: 0.0447 - val_acc: 0.9828\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0440 - acc: 0.9830 - val_loss: 0.0443 - val_acc: 0.9834\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0412 - acc: 0.9838 - val_loss: 0.0415 - val_acc: 0.9837\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0396 - acc: 0.9843 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0381 - acc: 0.9847 - val_loss: 0.0406 - val_acc: 0.9841\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0370 - acc: 0.9852 - val_loss: 0.0421 - val_acc: 0.9832\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0361 - acc: 0.9853 - val_loss: 0.0409 - val_acc: 0.9840\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0351 - acc: 0.9858 - val_loss: 0.0401 - val_acc: 0.9846\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0343 - acc: 0.9861 - val_loss: 0.0407 - val_acc: 0.9846\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0334 - acc: 0.9864 - val_loss: 0.0414 - val_acc: 0.9843\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0326 - acc: 0.9866 - val_loss: 0.0415 - val_acc: 0.9845\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0318 - acc: 0.9869 - val_loss: 0.0428 - val_acc: 0.9845\n",
      "Epoch 13/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0313 - acc: 0.9870 - val_loss: 0.0443 - val_acc: 0.9825\n",
      "Epoch 14/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0306 - acc: 0.9873 - val_loss: 0.0437 - val_acc: 0.9833\n",
      "In fold # 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0570 - acc: 0.9794 - val_loss: 0.0446 - val_acc: 0.9825\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0446 - acc: 0.9828 - val_loss: 0.0412 - val_acc: 0.9839\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0417 - acc: 0.9836 - val_loss: 0.0395 - val_acc: 0.9843\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0402 - acc: 0.9841 - val_loss: 0.0392 - val_acc: 0.9848\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0386 - acc: 0.9846 - val_loss: 0.0389 - val_acc: 0.9848\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0375 - acc: 0.9850 - val_loss: 0.0398 - val_acc: 0.9840\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0365 - acc: 0.9852 - val_loss: 0.0390 - val_acc: 0.9845\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0354 - acc: 0.9856 - val_loss: 0.0388 - val_acc: 0.9847\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0343 - acc: 0.9859 - val_loss: 0.0390 - val_acc: 0.9849\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0339 - acc: 0.9862 - val_loss: 0.0392 - val_acc: 0.9848\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0330 - acc: 0.9866 - val_loss: 0.0406 - val_acc: 0.9846\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0322 - acc: 0.9868 - val_loss: 0.0401 - val_acc: 0.9842\n",
      "Epoch 13/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0316 - acc: 0.9869 - val_loss: 0.0411 - val_acc: 0.9844\n",
      "Epoch 14/50\n",
      "143614/143614 [==============================] - 220s 2ms/step - loss: 0.0309 - acc: 0.9872 - val_loss: 0.0404 - val_acc: 0.9844\n",
      "In fold # 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0570 - acc: 0.9791 - val_loss: 0.0440 - val_acc: 0.9833\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0449 - acc: 0.9826 - val_loss: 0.0409 - val_acc: 0.9841\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0422 - acc: 0.9835 - val_loss: 0.0394 - val_acc: 0.9845\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0403 - acc: 0.9841 - val_loss: 0.0385 - val_acc: 0.9848\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0389 - acc: 0.9845 - val_loss: 0.0388 - val_acc: 0.9844\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0376 - acc: 0.9850 - val_loss: 0.0378 - val_acc: 0.9848\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0367 - acc: 0.9852 - val_loss: 0.0381 - val_acc: 0.9844\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0355 - acc: 0.9857 - val_loss: 0.0404 - val_acc: 0.9835\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0346 - acc: 0.9858 - val_loss: 0.0380 - val_acc: 0.9846\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0339 - acc: 0.9863 - val_loss: 0.0399 - val_acc: 0.9838\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0330 - acc: 0.9864 - val_loss: 0.0389 - val_acc: 0.9845\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0324 - acc: 0.9866 - val_loss: 0.0395 - val_acc: 0.9846\n",
      "In fold # 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0574 - acc: 0.9791 - val_loss: 0.0434 - val_acc: 0.9827\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0443 - acc: 0.9828 - val_loss: 0.0413 - val_acc: 0.9835\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0419 - acc: 0.9836 - val_loss: 0.0418 - val_acc: 0.9834\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0402 - acc: 0.9840 - val_loss: 0.0397 - val_acc: 0.9838\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 221s 2ms/step - loss: 0.0386 - acc: 0.9846 - val_loss: 0.0403 - val_acc: 0.9836\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 220s 2ms/step - loss: 0.0375 - acc: 0.9849 - val_loss: 0.0402 - val_acc: 0.9837\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 220s 2ms/step - loss: 0.0363 - acc: 0.9853 - val_loss: 0.0389 - val_acc: 0.9844\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0354 - acc: 0.9856 - val_loss: 0.0392 - val_acc: 0.9847\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0345 - acc: 0.9860 - val_loss: 0.0410 - val_acc: 0.9843\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0337 - acc: 0.9862 - val_loss: 0.0406 - val_acc: 0.9835\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0327 - acc: 0.9867 - val_loss: 0.0407 - val_acc: 0.9834\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0321 - acc: 0.9868 - val_loss: 0.0418 - val_acc: 0.9823\n",
      "Epoch 13/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0314 - acc: 0.9870 - val_loss: 0.0406 - val_acc: 0.9839\n",
      "In fold # 7\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 224s 2ms/step - loss: 0.0582 - acc: 0.9787 - val_loss: 0.0459 - val_acc: 0.9820\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0445 - acc: 0.9828 - val_loss: 0.0430 - val_acc: 0.9831\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0415 - acc: 0.9838 - val_loss: 0.0407 - val_acc: 0.9839\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0396 - acc: 0.9842 - val_loss: 0.0400 - val_acc: 0.9839\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0385 - acc: 0.9847 - val_loss: 0.0425 - val_acc: 0.9829\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0374 - acc: 0.9850 - val_loss: 0.0407 - val_acc: 0.9835\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0363 - acc: 0.9855 - val_loss: 0.0408 - val_acc: 0.9837\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0350 - acc: 0.9858 - val_loss: 0.0415 - val_acc: 0.9835\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0343 - acc: 0.9862 - val_loss: 0.0422 - val_acc: 0.9831\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0334 - acc: 0.9865 - val_loss: 0.0415 - val_acc: 0.9840\n",
      "In fold # 8\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 224s 2ms/step - loss: 0.0562 - acc: 0.9792 - val_loss: 0.0427 - val_acc: 0.9836\n",
      "Epoch 2/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0441 - acc: 0.9828 - val_loss: 0.0416 - val_acc: 0.9840\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0417 - acc: 0.9837 - val_loss: 0.0407 - val_acc: 0.9842\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0398 - acc: 0.9842 - val_loss: 0.0392 - val_acc: 0.9848\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0386 - acc: 0.9845 - val_loss: 0.0407 - val_acc: 0.9838\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0374 - acc: 0.9849 - val_loss: 0.0389 - val_acc: 0.9855\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0364 - acc: 0.9854 - val_loss: 0.0403 - val_acc: 0.9845\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0356 - acc: 0.9855 - val_loss: 0.0394 - val_acc: 0.9853\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0398 - val_acc: 0.9845\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0337 - acc: 0.9863 - val_loss: 0.0399 - val_acc: 0.9840\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0329 - acc: 0.9865 - val_loss: 0.0413 - val_acc: 0.9846\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0323 - acc: 0.9867 - val_loss: 0.0423 - val_acc: 0.9849\n",
      "In fold # 9\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/50\n",
      "143614/143614 [==============================] - 224s 2ms/step - loss: 0.0566 - acc: 0.9796 - val_loss: 0.0467 - val_acc: 0.9822\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0440 - acc: 0.9829 - val_loss: 0.0429 - val_acc: 0.9833\n",
      "Epoch 3/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0412 - acc: 0.9839 - val_loss: 0.0410 - val_acc: 0.9838\n",
      "Epoch 4/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0392 - acc: 0.9845 - val_loss: 0.0408 - val_acc: 0.9837\n",
      "Epoch 5/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0380 - acc: 0.9849 - val_loss: 0.0409 - val_acc: 0.9833\n",
      "Epoch 6/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0368 - acc: 0.9852 - val_loss: 0.0401 - val_acc: 0.9838\n",
      "Epoch 7/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0358 - acc: 0.9856 - val_loss: 0.0403 - val_acc: 0.9840\n",
      "Epoch 8/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0348 - acc: 0.9859 - val_loss: 0.0402 - val_acc: 0.9838\n",
      "Epoch 9/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0341 - acc: 0.9861 - val_loss: 0.0404 - val_acc: 0.9841\n",
      "Epoch 10/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0333 - acc: 0.9865 - val_loss: 0.0405 - val_acc: 0.9837\n",
      "Epoch 11/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0325 - acc: 0.9866 - val_loss: 0.0417 - val_acc: 0.9833\n",
      "Epoch 12/50\n",
      "143614/143614 [==============================] - 222s 2ms/step - loss: 0.0317 - acc: 0.9870 - val_loss: 0.0433 - val_acc: 0.9828\n"
     ]
    }
   ],
   "source": [
    "models, val_loss, total_auc, fold_predictions = train_folds(data, y, 10, 256, get_plain_attention_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall val-loss: 0.03917059923720672 AUC 0.989076054755853\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall val-loss:\", val_loss, \"AUC\", total_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall val-loss: 0.03917059923720672 AUC 0.989076054755853\n"
     ]
    }
   ],
   "source": [
    "print(\"Overall val-loss:\", val_loss, \"AUC\", total_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_fold_preditcions = np.concatenate(fold_predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training AUC 0.9886303316374511\n"
     ]
    }
   ],
   "source": [
    "training_auc = roc_auc_score(y[:-1], train_fold_preditcions)\n",
    "print(\"Training AUC\", training_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting testing results...\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n",
      "153164/153164 [==============================] - 85s 553us/step\n"
     ]
    }
   ],
   "source": [
    "#test_data = test_df\n",
    "CLASSES = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "submit_path_prefix = \"results/rnn/dropout-glove-bigru-attall-lp-ct-\" + str(MAX_SEQUENCE_LENGTH) \n",
    "\n",
    "print(\"Predicting testing results...\")\n",
    "test_predicts_list = []\n",
    "for fold_id, model in enumerate(models):\n",
    "    test_predicts = model.predict(test_data, batch_size=256, verbose=1)\n",
    "    test_predicts_list.append(test_predicts)\n",
    "    np.save(\"predict_path/\", test_predicts)\n",
    "\n",
    "test_predicts = np.zeros(test_predicts_list[0].shape)\n",
    "for fold_predict in test_predicts_list:\n",
    "    test_predicts += fold_predict\n",
    "test_predicts /= len(test_predicts_list)\n",
    "\n",
    "test_ids = test_df[\"id\"].values\n",
    "test_ids = test_ids.reshape((len(test_ids), 1))\n",
    "\n",
    "test_predicts = pd.DataFrame(data=test_predicts, columns=CLASSES)\n",
    "test_predicts[\"id\"] = test_ids\n",
    "test_predicts = test_predicts[[\"id\"] + CLASSES]\n",
    "submit_path = submit_path_prefix + \"-L{:4f}-A{:4f}.csv\".format(val_loss, total_auc)\n",
    "test_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting training results...\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicting training results...\")\n",
    "\n",
    "train_ids = train_df[\"id\"].values\n",
    "train_ids = train_ids.reshape((len(train_ids), 1))\n",
    "\n",
    "train_predicts = pd.DataFrame(data=train_fold_preditcions, columns=CLASSES) # IT MISS THE LAST ONE's label\n",
    "train_predicts[\"id\"] = train_ids[:-1]\n",
    "train_predicts = train_predicts[[\"id\"] + CLASSES]\n",
    "submit_path = submit_path_prefix + \"-Train-L{:4f}-A{:4f}.csv\".format(val_loss, training_auc)\n",
    "train_predicts.to_csv(submit_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
