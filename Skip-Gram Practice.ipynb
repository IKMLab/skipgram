{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing necessary preliminaries...\n",
      "Success.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import glovar\n",
    "import collections\n",
    "from data import toxic\n",
    "toxic.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Implementation Practice\n",
    "\n",
    "Your goal is to implement the Skip-Gram model in PyTorch, including pre-processing. \n",
    "\n",
    "Pre-processing is an important step in deep learning with text, and you should learn it now. Even though the Skip-Gram model is different from tasks such as sentence classification, these pre-processing skills are relevant to working with text in general. \n",
    "\n",
    "TO SKIP THE PRE-PROCESSING part of the tutorial, skip to number (5) and refer to the instructions. But it is recommended to make sure you are solid with these pre-processing skills.\n",
    "\n",
    "This tutorial assumes you are familiar with the basics of PyTorch. If not, you should only need review this tutorial first:\n",
    "\n",
    "https://github.com/jcjohnson/pytorch-examples\n",
    "\n",
    "Stages in this tutorial:\n",
    "1. Load the Data\n",
    "2. Tokenize the Data\n",
    "3. Build the Vocab Dictionary\n",
    "4. Prepare Training Pairs\n",
    "5. Implement Negative Sampling\n",
    "6. Prepare Data for the Network\n",
    "7. Code the Model\n",
    "8. Train the Model\n",
    "9. Visualize the Embeddings\n",
    "\n",
    "Tips are provided for the pre-processing stage to make it smoother. Try not to use them. If you have to, put aside some time to learn those skills properly.\n",
    "\n",
    "You will see validation cells along the way with `assert` calls. Run these to make sure you haven't made any mistakes along the way that will prevent you from proceeding. You will need to do the steps in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "We will use the data from Kaggle's Toxic Comment classification task. This data is very noisy, so we have selected a subset of the data that reduces this noise to make this tutorial easier.\n",
    "\n",
    "The files are provided in the `data` directory. They come in .csv format so we'll use pandas to handle them. If you haven't learned how to use pandas, do it! It is a very useful tool.\n",
    "\n",
    "The next cell defines the paths to these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_path = os.path.join(glovar.DATA_DIR, 'train.csv')\n",
    "test_file_path = os.path.join(glovar.DATA_DIR, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Do\n",
    "\n",
    "Load both .csv files into pandas `DataFrame` objects called `train` and `test` and see what they look like by calling the `head()` function on one of them. Identify the name of the column that contains the text of the comments. \n",
    "\n",
    "Tips:\n",
    "- If you don't know pandas, try using `pd.read_csv()`, but put aside some time to learn pandas properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = ???\n",
    "# test = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "We need to determine the set of all tokens in our dataset. We therefore need to separate each comment string into individual tokens, then determine the unique set of those tokens. We focus on the tokenization step first.\n",
    "\n",
    "We will use `nltk` for tokenization because it is lightweight. The `nltk` package defines a function called `word_tokenize()` that you can use.\n",
    "\n",
    "### To Do\n",
    "Given the `DataFrame`s above, obtain a `set` of all unique tokens from <strong>both</strong> the train and test sets. We need to account for all the data in our vocabulary. This variable should be called `token_set`.\n",
    "\n",
    "Tips:\n",
    "- The rows of `DataFrame` objects `df1` and `df1` can be joined together using `df1.append(df2)`\n",
    "- You can get an `array` of values in a `DataFrame` `df` column by calling `df['column_name'].values`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token_set = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(token_set) == len(toxic.get_token_set())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Vocab Dictionary\n",
    "\n",
    "We need to associate a unique `int` index with every unique token, and provide a map for lookup. A high-level view of text processing is often: \n",
    "1. receive text as input\n",
    "2. tokenize that text to obtain tokens\n",
    "3. use those tokens to obtain integer indices\n",
    "4. use those indices to lookup word vectors\n",
    "5. use those vectors as input to a neural network.\n",
    "\n",
    "We focus on (3) now.\n",
    "\n",
    "### To Do\n",
    "Use the `token_set` to build a `dict` object called `vocab` that has every unique token in the `token_set` as an index and unique integers as `values`. <strong>Also</strong>, since we will add a padding token for recurrent neural network processing later, make sure your values start from `1` and not `0` so that we can keep `0` for padding later.\n",
    "\n",
    "Tips:\n",
    "- The python `zip()` function can be used to bring two lists together - e.g. tokens and indices\n",
    "- The `dict()` constructor can take a zipped object as input, mapping the first position to index and second to value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vocab) == len(toxic.get_vocab())\n",
    "assert isinstance(list(vocab.keys())[0], str)\n",
    "assert isinstance(list(vocab.values())[0], int)\n",
    "assert 0 not in vocab.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a reverse lookup dictionary (integer indexes as keys and string tokens as values) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the Training Pairs\n",
    "\n",
    "We need to present two words at a time to the network to train our Skip-Gram: a center word and a context word. We therefore need to determine these pairs beforehand.\n",
    "\n",
    "Before coding deep learning models it is necessary to first fully think through how we are going to present the data to the network. This will avoid having to make annoying changes that might follow from small details that are easy to overlook.\n",
    "\n",
    "We know we are going to present two words at a time: a center word, and a context word. But how are we going to present them: as tokens, or as indices? These details matter when you code the forward pass of the network: if you try a word vector lookup on an embedding matrix with a string, you will see an error. We will use integer indices as it will be slightly faster than adding a dictionary lookup as well at training time.\n",
    "\n",
    "Since finding the context tokens for all words over all instances in the dataset is not a generally useful skill, we do that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5  # our context window size - experiment with this!\n",
    "contexts = toxic.get_contexts(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contexts` variable is a dictionary where the keys are the indices of all the tokens in the dataset, and the values are `set`s of token indices that occur in their contexts. We will sample from these during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the frequencies of our words for the negative sampling algorithm.\n",
    "\n",
    "### To Do\n",
    "\n",
    "Use the training data and `nltk.word_tokenize` to create a `collections.Counter` object that holds each unique token as a key, and the frequency count as a value.\n",
    "\n",
    "Tips:\n",
    "- `Counter` has an `update()` function that can accept lists of key values (i.e. tokens) and automatically does the counting for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(frequencies) == len(vocab)\n",
    "assert isinstance(list(frequencies.keys())[0], str)\n",
    "assert isinstance(list(frequencies.values())[0], int)\n",
    "rev_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to prepare our data for training is define it as a set of pairs of words. Making a complete pass over this set constitutes one epoch of the data.\n",
    "\n",
    "PyTorch provides a `torch.utils.data.dataset.Dataset` abstraction for a dataset. It is useful to use this abstraction and its companion the `torch.utils.data.dataloader.DataLoader`, which we will implement in (6). The `Dataset` class is abstract and we must be implemented. Actually, we could just use a list - the interface for a `Dataset` is `__len__(self)` and `__getitem__(self, ix)`, so a list will do here. At other times you might consider implementing a `Dataset`, for example if accessing the data dependended on the state of another object, or if you wanted to implement some more complicated training algorithm. We could also implement one that wrapped all the worked we did above - taking .csv files, tokenizing, preparing the vocab dict, etc - and made it conveniently applicable to any text dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3736855\n"
     ]
    }
   ],
   "source": [
    "pairs = set([])\n",
    "for center in contexts.keys():\n",
    "    pairs.update(tuple(zip([center] * len(contexts[center]), list(contexts[center]))))\n",
    "data = list(pairs)\n",
    "print('Number of pairs in the dataset: %s' % len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, once we multiply our ~80,000 unique tokens by their context sizes we have many training pairs to process. An efficient implementation with negative sampling will be very important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Negative Sampling\n",
    "\n",
    "If you are skipping pre-processing, uncomment the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = toxic.get_vocab()\n",
    "frequencies = toxic.get_frequencies()\n",
    "contexts = toxic.get_contexts(5)\n",
    "pairs = set([])\n",
    "for center in contexts.keys():\n",
    "    pairs.update(tuple(zip([center] * len(contexts[center]), list(contexts[center]))))\n",
    "pairs = list(pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform negative sampling, we need a function that\n",
    "- Takes a token index as argument\n",
    "- Returns the number of negative samples we desire\n",
    "- Randomly chooses those samples according to\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=0}^n (f(w_j)^{3/4})}\n",
    "$$\n",
    "\n",
    "We will define this function as a callable class, since it depends on state information (the `vocab`, `frequencies`, and `contexts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    \n",
    "    def __init__(self, vocab, frequencies, contexts, num_negs):\n",
    "        \"\"\"Create a new NegativeSampler.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary.\n",
    "          frequencies: List of integers, the frequencies of each word,\n",
    "            sorted in word index order.\n",
    "          contexts: Dictionary.\n",
    "          num_negs: Integer, how many to negatives to sample.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)\n",
    "        self.contexts = contexts\n",
    "        self.num_negs = num_negs\n",
    "        self.distribution = self.p(list(frequencies.values()))\n",
    "    \n",
    "    def __call__(self, tok_ix):\n",
    "        \"\"\"Get negative samples.\n",
    "        \n",
    "        Args:\n",
    "          tok_ix: Integer, the index of the center word.\n",
    "        \n",
    "        Returns:\n",
    "          List of integers.\n",
    "        \"\"\"\n",
    "        neg_samples = np.random.choice(\n",
    "            self.n, \n",
    "            size=self.num_negs, \n",
    "            p=self.distribution)\n",
    "        # make sure we haven't sampled center word or its context\n",
    "        invalid = [-1, tok_ix] + list(self.contexts[tok_ix])\n",
    "        for i, ix in enumerate(neg_samples):\n",
    "            if ix in invalid:\n",
    "                new_ix = -1\n",
    "                while new_ix in invalid:\n",
    "                    new_ix = np.random.choice(self.n, \n",
    "                                              size=1, \n",
    "                                              p=self.distribution)[0]\n",
    "                neg_samples[i] = new_ix\n",
    "        return [int(s) for s in neg_samples]\n",
    "    \n",
    "    def p(self, freqs):\n",
    "        \"\"\"Determine the probability distribution for negative sampling.\n",
    "        \n",
    "        Args:\n",
    "          freqs: List of integers.\n",
    "        \n",
    "        Returns:\n",
    "          numpy.ndarray.\n",
    "        \"\"\"\n",
    "        ### Impelement Me ###\n",
    "        freqs = np.array(freqs)\n",
    "        return np.power(freqs, 3/4) / np.sum(np.power(freqs, 3/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for the Network\n",
    "\n",
    "Whatever kind of data we pass our neural network, we often need to take some pre-processing steps at training and prediction time. In our case, we need to perform negative sampling and prepare whatever data structure we decide to pass to our network.\n",
    "\n",
    "What is in a batch of data?\n",
    "- center word\n",
    "- context word\n",
    "- negative samples\n",
    "\n",
    "How will we use it?\n",
    "- embedding lookup for all words\n",
    "- dot product of center word with context word and negative samples\n",
    "- softmax over the resulting values\n",
    "\n",
    "We must define our target vector for the cross-entropy loss calculation. Without negative sampling this is a probability distribution over the entire vocabulary with a `1` at the index of the context word. With negative sampling, we only want to calculate loss and backpropagate gradient for the words in each pass: context, negative samples. We can decide how to arrange this. A consistent convention will be convenient. With five negative samples, we will put the context word in the first position, so all our \"targets\" will look like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We show a row vector here, but in practice this will be a 1D `Tensor`.\n",
    "\n",
    "It will also be convenient to perform all our dot products in parallel. Efficient neural net implementations avoid for loops wherever possible. We can parallelize our implementation by stacking the output embeddings into a matrix and performing a matrix multiplication with the input vector.\n",
    "\n",
    "### To Do\n",
    "\n",
    "Define a `collate(pairs)` function that accepts a `list` of `tuple`s and returns an appropriate data structure, given the considerations just discussed. The tuples will all look like `(center, context)` - i.e. a pair in our `data` variable. The collate function will need to perform negative sampling. It will therefore depend on a NegativeSampler, so we should wrap it and make it a callable.\n",
    "\n",
    "This might be a challenge and you will need to think forward to how to perform the embedding lookup and matrix multiplication.\n",
    "\n",
    "Note: normally we would need to collate labels for training, but since we have a conventional target we don't need to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    \n",
    "    def __init__(self, neg_sampler):\n",
    "        self.sampler = neg_sampler\n",
    "    \n",
    "    def __call__(self, pairs):\n",
    "        ### Implement Me ###\n",
    "        batch_size = len(pairs)\n",
    "        centers = [x[0] for x in pairs]\n",
    "        contexts = [x[1] for x in pairs]\n",
    "        context_and_negs = []\n",
    "        for i in range(batch_size):\n",
    "            neg_samples = self.sampler(centers[i])\n",
    "            context_and_negs.append([contexts[i]] + list(neg_samples))\n",
    "        return centers, context_and_negs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will want to create a `DataLoader` with our collate function. This will take care of some nice things for us: \n",
    "- shuffling our training pairs each epoch\n",
    "- facilitating enumeration of batches for training\n",
    "- applying our collate function to each batch\n",
    "- parallelizing this work on the CPU whilst our GPU processes our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data, batch_size, collate_fn):\n",
    "    return dataloader.DataLoader(data, \n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=1, \n",
    "                                 collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Code the Model\n",
    "\n",
    "For each training pair, we need to:\n",
    "1. Perform embedding lookup for the center word (from $\\mathbf{V}$)\n",
    "2. Lookup the context and negative sample embeddings (from $\\mathbf{U}$)\n",
    "3. Perform a dot product of the center word embeddings with context and negative sample embeddings\n",
    "4. Pass the results of the dot products to the softmax function\n",
    "5. Compute the loss given the labels\n",
    "6. Use the loss to update the vectors\n",
    "\n",
    "Below is a template for the model. We will walk through the steps here. Look for `### Implement Me ###` in the model template for the parts you need to complete.\n",
    "\n",
    "### To Do\n",
    "\n",
    "1. Define the network parameters in the `__init__` method. Specifically we need $\\mathbf{V} \\in \\Bbb{R}^{\\text{vocab size} \\times \\text{emb_dim}}$ and $\\mathbf{U} \\in \\Bbb{R}^{\\text{emb_dim} \\times \\text{vocab size}}$. Define them as `nn.Parameter` objects so that PyTorch will automatically add them to `self.parameters()` on the `nn.Module` to make it easier to work with.\n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"SkipGram Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, emb_dim, num_negs, lr):\n",
    "        \"\"\"Create a new SkipGram.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary, our vocab dict with token keys and index values.\n",
    "          emb_dim: Integer, the size of word embeddings.\n",
    "          num_negs: Integer, the number of non-context words to sample.\n",
    "          lr: Float, the learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)  # size of the vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_negs = num_negs\n",
    "        \n",
    "        ### Implement Me: define V and U ###\n",
    "        \n",
    "        ### Implement Me: initialize V and U with unform distribution in [-0.01, 0.01] ###\n",
    "        \n",
    "        self.V = nn.Embedding(self.n, emb_dim)\n",
    "        self.U = nn.Embedding(self.n, emb_dim)\n",
    "        #self.V = nn.Parameter(torch.Tensor(self.n, emb_dim), requires_grad=True)\n",
    "        #self.U = nn.Parameter(torch.Tensor(emb_dim, self.n), requires_grad=True)\n",
    "        nn.init.uniform(self.V.weight, a=-0.01, b=0.01)\n",
    "        nn.init.uniform(self.U.weight, a=-0.01, b=0.01)        \n",
    "        \n",
    "        # Adam is a good optimizer and will converge faster than SGD\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        \"\"\"Compute the forward pass of the network.\n",
    "        \n",
    "        1. Lookup embeddings for center and context words\n",
    "            - use self.lookup()\n",
    "        2. Sample negative samples\n",
    "            - use self.negative_samples()\n",
    "        2. Calculate the probability estimates\n",
    "            - make sure to implement and use self.softmax()\n",
    "        3. Calculate the loss\n",
    "            - using self.loss()\n",
    "        \n",
    "        Args:\n",
    "          batch: whatever data structure you decided to make for a batch.\n",
    "        \n",
    "        Returns:\n",
    "          loss (torch.autograd.Variable).\n",
    "        \"\"\"\n",
    "        ### Implement Me: lookup center words ###\n",
    "        ### Implement Me: lookup context words ###\n",
    "        ### Implement Me: lookup negative words ###\n",
    "        cents, conts_negs = batch\n",
    "        cents = self.V(self.lookup_tensor(cents))\n",
    "        conts_negs = self.U(self.lookup_tensor(conts_negs)).permute([0, 2, 1])\n",
    "        logits = cents.matmul(conts_negs)\n",
    "        preds = self.softmax(logits)\n",
    "        targets = self.targets(cents.shape[0])\n",
    "        return self.loss(preds, targets)\n",
    "    \n",
    "    def lookup_tensor(self, indices):\n",
    "        \"\"\"Lookup embeddings given indices.\n",
    "        \n",
    "        Args:\n",
    "          embedding: nn.Parameter, an embedding matrix.\n",
    "          indices: List of integers, the indices to lookup.\n",
    "        \n",
    "        Returns:\n",
    "          torch.autograd.Variable of shape [len(indices), emb_dim]. A matrix \n",
    "            with horizontally stacked word vectors.\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(torch.LongTensor(indices),\n",
    "                            requires_grad=False).cuda()\n",
    "        else:\n",
    "            return Variable(torch.LongTensor(indices),\n",
    "                            requires_grad=False)\n",
    "    \n",
    "    def loss(self, preds, targets):\n",
    "        \"\"\"Compute cross-entropy loss.\n",
    "        \n",
    "        Implement this for practice, don't use the built-in PyTorch function.\n",
    "        \n",
    "        Args:\n",
    "          preds: Tensor of shape [batch_size, vocab_size], our predictions.\n",
    "          targets: List of integers, the vocab indices of target context words.\n",
    "        \"\"\"\n",
    "        ### Implement Me ###\n",
    "        return -1 * torch.sum(targets * torch.log(preds))\n",
    "    \n",
    "    def optimize(self, loss):\n",
    "        \"\"\"Optimization step.\n",
    "        \n",
    "        Args:\n",
    "          loss: Scalar.\n",
    "        \"\"\"\n",
    "        # Remove any previous gradient from our tensors before calculating again.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()        \n",
    "    \n",
    "    def softmax(self, logits):\n",
    "        \"\"\"Compute the softmax function.\n",
    "        \n",
    "        Implement this for practice, don't use the built-in PyTorch function.\n",
    "        \n",
    "        Args:\n",
    "          logits: Tensor of shape [batch_size, vocab_size].\n",
    "        \n",
    "        Returns:\n",
    "          Tensor of shape [batch_size, vocab_size], our predictions.\n",
    "        \"\"\"\n",
    "        ### Impelement Me ###\n",
    "        return torch.exp(logits) / torch.sum(torch.exp(logits))\n",
    "    \n",
    "    def targets(self, batch_size):\n",
    "        \"\"\"Get the conventional targets for the batch.\n",
    "        \n",
    "        Args:\n",
    "          batch_size: Integer.\n",
    "        \n",
    "        Returns:\n",
    "          torch.LongTensor.\n",
    "        \"\"\"\n",
    "        targets = torch.zeros((batch_size, self.num_negs + 1))\n",
    "        targets[:, 0] = 1\n",
    "        return Variable(targets, requires_grad=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "Training algorithms should consider\n",
    "- Learning rate annealing\n",
    "- Convergence conditions\n",
    "- Early stopping\n",
    "\n",
    "Here we will not anneal the learning rate for simplicity and pre-define a 10 epoch limit. Training will stop early if loss doesn't improve after one epoch.\n",
    "\n",
    "You should experiment with different training strategies in your own work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Step 0\t\tLoss 1878.2468\n",
      "Step 100\t\tLoss 1881.4657\n",
      "Step 200\t\tLoss 1879.0082\n",
      "Step 300\t\tLoss 1881.7604\n",
      "Step 400\t\tLoss 1892.5626\n",
      "Step 500\t\tLoss 1909.8878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-17:\n",
      "  File \"/home/hanshan/anaconda3/envs/py35/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hanshan/anaconda3/envs/py35/lib/python3.5/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/hanshan/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 42, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"<ipython-input-5-0aced094a953>\", line 13, in __call__\n",
      "    neg_samples = self.sampler(centers[i])\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-11-307ab89c14cd>\", line 35, in __call__\n",
      "    if ix in invalid:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-5aac21f77d53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrcvd_idx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36mrecv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmaxlength\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"negative maxlength\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bad_message_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmaxsize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "epoch = 0\n",
    "max_epochs = 10\n",
    "emb_dim = 50\n",
    "num_negs = 5\n",
    "lr = 0.02\n",
    "batch_size = 16\n",
    "\n",
    "sampler = NegativeSampler(vocab, frequencies, contexts, num_negs)\n",
    "collate = Collate(sampler)\n",
    "data_loader = get_data_loader(pairs, batch_size, collate)\n",
    "model = SkipGram(vocab, emb_dim, num_negs, lr)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "while epoch <= max_epochs:\n",
    "    epoch += 1\n",
    "    print('Epoch %s' % epoch)\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        loss = model.forward(batch)\n",
    "        model.optimize(loss)\n",
    "        if step % 100 == 0:\n",
    "            print('Step %s\\t\\tLoss %s' % (step, loss.data.cpu().numpy()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py35)",
   "language": "python",
   "name": "conda_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
