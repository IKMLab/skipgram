{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import glovar\n",
    "import collections\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Implementation Practice\n",
    "\n",
    "Your goal is to implement the Skip-Gram model in PyTorch, including pre-processing. \n",
    "\n",
    "Pre-processing is an important step in deep learning with text, and you should learn it now. Even though the Skip-Gram model is different from tasks such as sentence classification, these pre-processing skills are relevant to working with text in general. \n",
    "\n",
    "This tutorial assumes you are familiar with the basics of PyTorch. If not, you should only need review this tutorial first:\n",
    "\n",
    "https://github.com/jcjohnson/pytorch-examples\n",
    "\n",
    "Stages in this tutorial:\n",
    "1. Load the Data\n",
    "2. Tokenize the Data\n",
    "3. Build the Vocab Dictionary\n",
    "4. Prepare Training Pairs\n",
    "5. Implement Negative Sampling\n",
    "6. Prepare Data for the Network\n",
    "7. Code the Model\n",
    "8. Train the Model\n",
    "9. Visualize the Embeddings\n",
    "10. Use the Embeddings\n",
    "\n",
    "Tips are provided for the pre-processing stage to make it smoother. Try not to use them. If you have to, put aside some time to learn those skills properly.\n",
    "\n",
    "You will see validation cells along the way with `assert` calls. Run these to make sure you haven't made any mistakes along the way that will prevent you from proceeding. You will need to do the steps in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "We will use the data from SemEval18 Task 12, the Argument Reasoning Comprehension Task (https://github.com/habernal/semeval2018-task12).\n",
    "\n",
    "The data comes in csv format so we will use pandas to load the data. If you don't know pandas, you should get to know it right away because it is extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(glovar.DATA_DIR, 'train-full.txt')\n",
    "dev_path = os.path.join(glovar.DATA_DIR, 'dev-full.txt')\n",
    "test_path = os.path.join(glovar.DATA_DIR, 'test-only-data.txt')\n",
    "test_labels_path = os.path.join(glovar.DATA_DIR, 'test-labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path, delimiter='\\t')\n",
    "dev = pd.read_csv(dev_path, delimiter='\\t')\n",
    "test = pd.read_csv(test_path, delimiter='\\t')\n",
    "test_labels = pd.read_csv(test_labels_path, delimiter='\\t', header=None)\n",
    "test_labels.columns = ['#id', 'correctLabelW0orW1']\n",
    "test = pd.concat([test, test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "We need to determine the set of all tokens in our dataset. We therefore need to separate each comment string into individual tokens, then determine the unique set of those tokens. We focus on the tokenization step first.\n",
    "\n",
    "We will use `nltk` for tokenization because it is lightweight. The `nltk` package defines a function called `word_tokenize()` that is useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['claim', 'reason', 'debateInfo', 'debateTitle', 'warrant0', 'warrant1']\n",
    "sents = []\n",
    "for column in text_columns:\n",
    "    sents += list(train[column].values)\n",
    "    sents += list(dev[column].values)\n",
    "    sents += list(test[column].values)\n",
    "# Some values are nan - remove those\n",
    "sents = [s for s in sents if not pd.isnull(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement Me: determine the token set ##\n",
    "# token_set = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(token_set) == 6116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Vocab Dictionary\n",
    "\n",
    "We need to associate a unique `int` index with every unique token, and provide a map for lookup. A high-level view of text processing is often: \n",
    "1. receive text as input\n",
    "2. tokenize that text to obtain tokens\n",
    "3. map those tokens to integer indices\n",
    "4. use those indices to lookup word vectors\n",
    "5. use those vectors as input to a neural network.\n",
    "\n",
    "We focus on (3) now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement Me: make the vocab dict, and add <PAD> ##\n",
    "# vocab = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vocab) == 6117\n",
    "assert isinstance(list(vocab.keys())[0], str)\n",
    "assert isinstance(list(vocab.values())[0], int)\n",
    "assert '<PAD>' in vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a reverse lookup dictionary (integer indexes as keys and string tokens as values) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the Training Pairs\n",
    "\n",
    "We need to present two words at a time to the network to train our Skip-Gram: a center word and a context word. We therefore need to determine these pairs beforehand.\n",
    "\n",
    "Before coding deep learning models it is necessary to first fully think through how we are going to present the data to the network. This will avoid having to make annoying changes that might follow from small details that are easy to overlook.\n",
    "\n",
    "We know we are going to present two words at a time: a center word, and a context word. But how are we going to present them: as tokens, or as indices? These details matter when you code the forward pass of the network: if you try a word vector lookup on an embedding matrix with a string, you will see an error. We will use integer indices as it will be slightly faster than adding a dictionary lookup as well at training time.\n",
    "\n",
    "Since finding the context tokens for all words over all instances in the dataset is not a generally useful skill, we do that for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, however, we will count the frequencies of the tokens in the dataset for our negative sampling algorithm.\n",
    "\n",
    "If we wanted we could use these frequencies to subsample frequent words when determining our training pairs. We won't do that here, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Implement Me: count frequencies of tokens in a collections.Counter object ###\n",
    "# frequencies = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll add <PAD> to the frequencies to even the lengths of the probability distributions for later\n",
    "frequencies['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(frequencies) == len(vocab)\n",
    "assert isinstance(list(frequencies.keys())[0], str)\n",
    "assert isinstance(list(frequencies.values())[0], int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll determine our pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5  # our context window size - you can experiment with this\n",
    "contexts = {k: set() for k in range(1, len(vocab) + 1)}\n",
    "for sent in sents:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    for i, center in enumerate(tokens):\n",
    "        center = vocab[center]\n",
    "        left_context = [vocab[t] for t in tokens[max(0, i - m):i - 1]]\n",
    "        right_context = [vocab[t] for t in tokens[i + 1: min(len(tokens), i + m)]]\n",
    "        contexts[center].update(left_context + right_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contexts` variable is a dictionary where the keys are the indices of all the tokens in the dataset, and the values are `set`s of token indices that occur in their contexts. We will sample from these during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to prepare our data for training is define it as a set of pairs of words. Making a complete pass over this set constitutes one epoch of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = set([])\n",
    "for center in contexts.keys():\n",
    "    pairs.update(tuple(zip([center] * len(contexts[center]), list(contexts[center]))))\n",
    "data = list(pairs)\n",
    "print('Number of pairs in the dataset: %s' % len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Negative Sampling\n",
    "\n",
    "To perform negative sampling, we need a function that\n",
    "- Takes a token index as argument\n",
    "- Returns the number of negative samples we desire\n",
    "- Randomly chooses those samples according to\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=0}^n (f(w_j)^{3/4})}\n",
    "$$\n",
    "\n",
    "We will define this function as a callable class, since it depends on state information (the `vocab`, `frequencies`, and `contexts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    \n",
    "    def __init__(self, vocab, frequencies, contexts, num_negs):\n",
    "        \"\"\"Create a new NegativeSampler.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary.\n",
    "          frequencies: List of integers, the frequencies of each word,\n",
    "            sorted in word index order.\n",
    "          contexts: Dictionary.\n",
    "          num_negs: Integer, how many to negatives to sample.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)\n",
    "        self.contexts = contexts\n",
    "        self.num_negs = num_negs\n",
    "        self.distribution = self.p(list(frequencies.values()))\n",
    "    \n",
    "    def __call__(self, tok_ix):\n",
    "        \"\"\"Get negative samples.\n",
    "        \n",
    "        Args:\n",
    "          tok_ix: Integer, the index of the center word.\n",
    "        \n",
    "        Returns:\n",
    "          List of integers.\n",
    "        \"\"\"\n",
    "        neg_samples = np.random.choice(\n",
    "            self.n, \n",
    "            size=self.num_negs, \n",
    "            p=self.distribution)\n",
    "        # make sure we haven't sampled center word or its context\n",
    "        invalid = [-1, tok_ix] + list(self.contexts[tok_ix])\n",
    "        for i, ix in enumerate(neg_samples):\n",
    "            if ix in invalid:\n",
    "                new_ix = -1\n",
    "                while new_ix in invalid:\n",
    "                    new_ix = np.random.choice(self.n, \n",
    "                                              size=1, \n",
    "                                              p=self.distribution)[0]\n",
    "                neg_samples[i] = new_ix\n",
    "        return [int(s) for s in neg_samples]\n",
    "    \n",
    "    def p(self, freqs):\n",
    "        \"\"\"Determine the probability distribution for negative sampling.\n",
    "        \n",
    "        Args:\n",
    "          freqs: List of integers.\n",
    "        \n",
    "        Returns:\n",
    "          numpy.ndarray.\n",
    "        \"\"\"\n",
    "        freqs = np.array(freqs)\n",
    "        return np.power(freqs, 3/4) / np.sum(np.power(freqs, 3/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for the Network\n",
    "\n",
    "Whatever kind of data we pass our neural network, we often need to take some pre-processing steps at training and prediction time. In our case, we need to perform negative sampling and prepare whatever data structure we decide to pass to our network.\n",
    "\n",
    "What is in a batch of data?\n",
    "- center word\n",
    "- context word\n",
    "- negative samples\n",
    "\n",
    "How will we use it?\n",
    "- embedding lookup for all words\n",
    "- dot product of center word with context word and negative samples\n",
    "- softmax over the resulting values\n",
    "\n",
    "We must define our target vector for the cross-entropy loss calculation. Without negative sampling this is a probability distribution over the entire vocabulary with a `1` at the index of the context word. With negative sampling, we only want to calculate loss and backpropagate gradient for the words in each pass: context, negative samples. We can decide how to arrange this. A consistent convention will be convenient. With five negative samples, we will put the context word in the first position, so all our \"targets\" will look like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We show a row vector here, but in practice this will be a 1D `Tensor`.\n",
    "\n",
    "It will also be convenient to perform all our dot products in parallel. Efficient neural net implementations avoid for loops wherever possible. We can parallelize our implementation by stacking the output embeddings into a matrix and performing a matrix multiplication with the input vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    \n",
    "    def __init__(self, neg_sampler):\n",
    "        self.sampler = neg_sampler\n",
    "    \n",
    "    def __call__(self, pairs):\n",
    "        batch_size = len(pairs)\n",
    "        centers = [x[0] for x in pairs]\n",
    "        contexts = [x[1] for x in pairs]\n",
    "        context_and_negs = []\n",
    "        for i in range(batch_size):\n",
    "            neg_samples = self.sampler(centers[i])\n",
    "            context_and_negs.append([contexts[i]] + list(neg_samples))\n",
    "        return centers, context_and_negs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will want to create a `DataLoader` with our collate function. This will take care of some nice things for us: \n",
    "- shuffling our training pairs each epoch\n",
    "- facilitating enumeration of batches for training\n",
    "- applying our collate function to each batch\n",
    "- parallelizing this work on the CPU whilst our GPU processes our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data, batch_size, collate_fn):\n",
    "    return dataloader.DataLoader(data, \n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=1, \n",
    "                                 collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Code the Model\n",
    "\n",
    "For each training pair, we need to:\n",
    "1. Perform embedding lookup for the center word (from $\\mathbf{V}$)\n",
    "2. Lookup the context and negative sample embeddings (from $\\mathbf{U}$)\n",
    "3. Perform a dot product of the center word embeddings with context and negative sample embeddings\n",
    "4. Pass the results of the dot products to the softmax function\n",
    "5. Compute the loss given the labels\n",
    "6. Use the loss to update the vectors\n",
    "\n",
    "Below is a template for the model. We will walk through the steps here. Look for `### Implement Me ###` in the model template for the parts you need to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"SkipGram Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, emb_dim, num_negs, lr):\n",
    "        \"\"\"Create a new SkipGram.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary, our vocab dict with token keys and index values.\n",
    "          emb_dim: Integer, the size of word embeddings.\n",
    "          num_negs: Integer, the number of non-context words to sample.\n",
    "          lr: Float, the learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)  # size of the vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_negs = num_negs\n",
    "        \n",
    "        ### Implement Me: define V and U ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        ### Implement Me: initialize V and U with unform distribution in [-1., 1.] ###\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Adam is a good optimizer and will converge faster than SGD\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "    \n",
    "    def forward(self, centers, context_and_negs):\n",
    "        \"\"\"Compute the forward pass of the network.\n",
    "        \n",
    "        Args:\n",
    "          centers: List of integers.\n",
    "          context_and_negs: List of integers.\n",
    "        \n",
    "        Returns:\n",
    "          loss (torch.autograd.Variable).\n",
    "        \"\"\"\n",
    "        ### Implement Me: lookup centers ###\n",
    "        \n",
    "        \n",
    "        ### Implement Me: lookup contextand_negs ###\n",
    "        \n",
    "        \n",
    "        ### Implement Me: calculate the probability distribution and calculate the loss ###\n",
    "        \n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def lookup_tensor(self, indices):\n",
    "        \"\"\"Lookup embeddings given indices.\n",
    "        \n",
    "        Args:\n",
    "          embedding: nn.Parameter, an embedding matrix.\n",
    "          indices: List of integers, the indices to lookup.\n",
    "        \n",
    "        Returns:\n",
    "          torch.autograd.Variable of shape [len(indices), emb_dim]. A matrix \n",
    "            with horizontally stacked word vectors.\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(torch.LongTensor(indices),\n",
    "                            requires_grad=False).cuda()\n",
    "        else:\n",
    "            return Variable(torch.LongTensor(indices),\n",
    "                            requires_grad=False)\n",
    "\n",
    "    def optimize(self, loss):\n",
    "        \"\"\"Optimization step.\n",
    "        \n",
    "        Args:\n",
    "          loss: Scalar.\n",
    "        \"\"\"\n",
    "        # Remove any previous gradient from our tensors before calculating again.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "Training algorithms should consider\n",
    "- Learning rate annealing\n",
    "- Convergence conditions\n",
    "- Early stopping\n",
    "\n",
    "Here we will not anneal the learning rate for simplicity and pre-define a 10 epoch limit. Training will stop early if loss doesn't improve after one epoch.\n",
    "\n",
    "You should experiment with different training strategies in your own work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "max_epochs = 5\n",
    "emb_dim = 50\n",
    "num_negs = 20\n",
    "lr = 0.01\n",
    "batch_size = 16\n",
    "\n",
    "sampler = NegativeSampler(vocab, frequencies, contexts, num_negs)\n",
    "collate = Collate(sampler)\n",
    "data_loader = get_data_loader(list(pairs), batch_size, collate)\n",
    "model = SkipGram(vocab, emb_dim, num_negs, lr)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "epoch = 0\n",
    "global_step = 0\n",
    "cum_loss = 0.\n",
    "while epoch < max_epochs:\n",
    "    epoch += 1\n",
    "    print('Epoch %s' % epoch)\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        global_step  += 1\n",
    "        loss = model.forward(*batch)\n",
    "        model.optimize(loss)\n",
    "        loss = loss.data.cpu().numpy()[0]\n",
    "        cum_loss += loss\n",
    "        if step % 1000 == 0:\n",
    "            print('Step %s\\t\\tLoss %8.4f' % (step, cum_loss / (global_step * batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the hyperparameters. You should see the average accumulated loss steadily decrease and start to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Embeddings\n",
    "\n",
    "We use TSNE to learn a 2D representation of our vectors. We then randomly sample 100 words to plot. Hopefully you will notice some similar words close together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ixs = torch.from_numpy(np.random.choice(range(len(vocab)), 100)).long()\n",
    "embeddings = model.V.weight[random_ixs].data.cpu().numpy()\n",
    "tok_ixs = {k: v for k, v in vocab.items() if v in random_ixs}\n",
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(embeddings)\n",
    "df = pd.DataFrame(X_tsne, index=tok_ixs, columns=['x', 'y'])\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py35)",
   "language": "python",
   "name": "conda_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
