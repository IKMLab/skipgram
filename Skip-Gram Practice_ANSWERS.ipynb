{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import dataset, dataloader\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import glovar\n",
    "import collections\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-Gram Implementation Practice\n",
    "\n",
    "Your goal is to implement the Skip-Gram model in PyTorch, including pre-processing. \n",
    "\n",
    "Pre-processing is an important step in deep learning with text, and you should learn it now. Even though the Skip-Gram model is different from tasks such as sentence classification, these pre-processing skills are relevant to working with text in general. \n",
    "\n",
    "This tutorial assumes you are familiar with the basics of PyTorch. If not, you should only need review this tutorial first:\n",
    "\n",
    "https://github.com/jcjohnson/pytorch-examples\n",
    "\n",
    "Stages in this tutorial:\n",
    "1. Load the Data\n",
    "2. Tokenize the Data\n",
    "3. Build the Vocab Dictionary\n",
    "4. Prepare Training Pairs\n",
    "5. Implement Negative Sampling\n",
    "6. Prepare Data for the Network\n",
    "7. Code the Model\n",
    "8. Train the Model\n",
    "9. Visualize the Embeddings\n",
    "10. Use the Embeddings\n",
    "\n",
    "Tips are provided for the pre-processing stage to make it smoother. Try not to use them. If you have to, put aside some time to learn those skills properly.\n",
    "\n",
    "You will see validation cells along the way with `assert` calls. Run these to make sure you haven't made any mistakes along the way that will prevent you from proceeding. You will need to do the steps in order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the Data\n",
    "\n",
    "We will use the data from SemEval18 Task 12, the Argument Reasoning Comprehension Task (https://github.com/habernal/semeval2018-task12).\n",
    "\n",
    "The data comes in csv format so we will use pandas to load the data. If you don't know pandas, you should get to know it right away because it is extremely useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join(glovar.DATA_DIR, 'train-full.txt')\n",
    "dev_path = os.path.join(glovar.DATA_DIR, 'dev-full.txt')\n",
    "test_path = os.path.join(glovar.DATA_DIR, 'test-only-data.txt')\n",
    "test_labels_path = os.path.join(glovar.DATA_DIR, 'test-labels.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(train_path, delimiter='\\t')\n",
    "dev = pd.read_csv(dev_path, delimiter='\\t')\n",
    "test = pd.read_csv(test_path, delimiter='\\t')\n",
    "test_labels = pd.read_csv(test_labels_path, delimiter='\\t', header=None)\n",
    "test_labels.columns = ['#id', 'correctLabelW0orW1']\n",
    "test = pd.concat([test, test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "We need to determine the set of all tokens in our dataset. We therefore need to separate each comment string into individual tokens, then determine the unique set of those tokens. We focus on the tokenization step first.\n",
    "\n",
    "We will use `nltk` for tokenization because it is lightweight. The `nltk` package defines a function called `word_tokenize()` that is useful for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_columns = ['claim', 'reason', 'debateInfo', 'debateTitle', 'warrant0', 'warrant1']\n",
    "sents = []\n",
    "for column in text_columns:\n",
    "    sents += list(train[column].values)\n",
    "    sents += list(dev[column].values)\n",
    "    sents += list(test[column].values)\n",
    "# Some values are nan - remove those\n",
    "sents = [s for s in sents if not pd.isnull(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_set = set([])\n",
    "for sent in sents:\n",
    "    token_set.update(nltk.word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(token_set) == 6116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build the Vocab Dictionary\n",
    "\n",
    "We need to associate a unique `int` index with every unique token, and provide a map for lookup. A high-level view of text processing is often: \n",
    "1. receive text as input\n",
    "2. tokenize that text to obtain tokens\n",
    "3. map those tokens to integer indices\n",
    "4. use those indices to lookup word vectors\n",
    "5. use those vectors as input to a neural network.\n",
    "\n",
    "We focus on (3) now.\n",
    "\n",
    "### To Do\n",
    "Use the `token_set` to build a `dict` object called `vocab` that has every unique token in the `token_set` as an index and unique integers as `values`. Also add a token `'<PAD>'` to the vocab. We will need it when we deal with recurrent neural networks.\n",
    "\n",
    "Tips:\n",
    "- The python `zip()` function can be used to bring two lists together - e.g. tokens and indices\n",
    "- The `dict()` constructor can take a zipped object as input, mapping the first position to index and second to value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = ???\n",
    "vocab = dict(zip(['<PAD>'] + list(token_set), \n",
    "                 range(len(token_set) + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(vocab) == 6117\n",
    "assert isinstance(list(vocab.keys())[0], str)\n",
    "assert isinstance(list(vocab.values())[0], int)\n",
    "assert '<PAD>' in vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define a reverse lookup dictionary (integer indexes as keys and string tokens as values) for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare the Training Pairs\n",
    "\n",
    "We need to present two words at a time to the network to train our Skip-Gram: a center word and a context word. We therefore need to determine these pairs beforehand.\n",
    "\n",
    "Before coding deep learning models it is necessary to first fully think through how we are going to present the data to the network. This will avoid having to make annoying changes that might follow from small details that are easy to overlook.\n",
    "\n",
    "We know we are going to present two words at a time: a center word, and a context word. But how are we going to present them: as tokens, or as indices? These details matter when you code the forward pass of the network: if you try a word vector lookup on an embedding matrix with a string, you will see an error. We will use integer indices as it will be slightly faster than adding a dictionary lookup as well at training time.\n",
    "\n",
    "Since finding the context tokens for all words over all instances in the dataset is not a generally useful skill, we do that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 5  # our context window size - you can experiment with this\n",
    "contexts = {k: set() for k in range(1, len(vocab) + 1)}\n",
    "for sent in sents:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    for i, center in enumerate(tokens):\n",
    "        center = vocab[center]\n",
    "        left_context = [vocab[t] for t in tokens[max(0, i - m):i - 1]]\n",
    "        right_context = [vocab[t] for t in tokens[i + 1: min(len(tokens), i + m)]]\n",
    "        contexts[center].update(left_context + right_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `contexts` variable is a dictionary where the keys are the indices of all the tokens in the dataset, and the values are `set`s of token indices that occur in their contexts. We will sample from these during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need the frequencies of our words for the negative sampling algorithm.\n",
    "\n",
    "### To Do\n",
    "\n",
    "Use the training data and `nltk.word_tokenize` to create a `collections.Counter` object that holds each unique token as a key, and the frequency count as a value.\n",
    "\n",
    "Tips:\n",
    "- `Counter` has an `update()` function that can accept lists of key values (i.e. tokens) and automatically does the counting for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequencies = ???\n",
    "frequencies = collections.Counter()\n",
    "for sent in sents:\n",
    "    tokens = nltk.word_tokenize(sent)\n",
    "    frequencies.update(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(frequencies) == len(vocab) - 1  # we don't see <PAD> in the data\n",
    "assert isinstance(list(frequencies.keys())[0], str)\n",
    "assert isinstance(list(frequencies.values())[0], int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll add <PAD> to the frequencies to even the lengths of the probability distributions for later\n",
    "frequencies['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to prepare our data for training is define it as a set of pairs of words. Making a complete pass over this set constitutes one epoch of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pairs in the dataset: 182776\n"
     ]
    }
   ],
   "source": [
    "pairs = set([])\n",
    "for center in contexts.keys():\n",
    "    pairs.update(tuple(zip([center] * len(contexts[center]), list(contexts[center]))))\n",
    "data = list(pairs)\n",
    "print('Number of pairs in the dataset: %s' % len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Negative Sampling\n",
    "\n",
    "To perform negative sampling, we need a function that\n",
    "- Takes a token index as argument\n",
    "- Returns the number of negative samples we desire\n",
    "- Randomly chooses those samples according to\n",
    "\n",
    "$$\n",
    "P(w_i) = \\frac{f(w_i)^{3/4}}{\\sum_{j=0}^n (f(w_j)^{3/4})}\n",
    "$$\n",
    "\n",
    "We will define this function as a callable class, since it depends on state information (the `vocab`, `frequencies`, and `contexts`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler:\n",
    "    \n",
    "    def __init__(self, vocab, frequencies, contexts, num_negs):\n",
    "        \"\"\"Create a new NegativeSampler.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary.\n",
    "          frequencies: List of integers, the frequencies of each word,\n",
    "            sorted in word index order.\n",
    "          contexts: Dictionary.\n",
    "          num_negs: Integer, how many to negatives to sample.\n",
    "        \"\"\"\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)\n",
    "        self.contexts = contexts\n",
    "        self.num_negs = num_negs\n",
    "        self.distribution = self.p(list(frequencies.values()))\n",
    "    \n",
    "    def __call__(self, tok_ix):\n",
    "        \"\"\"Get negative samples.\n",
    "        \n",
    "        Args:\n",
    "          tok_ix: Integer, the index of the center word.\n",
    "        \n",
    "        Returns:\n",
    "          List of integers.\n",
    "        \"\"\"\n",
    "        neg_samples = np.random.choice(\n",
    "            self.n, \n",
    "            size=self.num_negs, \n",
    "            p=self.distribution)\n",
    "        # make sure we haven't sampled center word or its context\n",
    "        invalid = [-1, tok_ix] + list(self.contexts[tok_ix])\n",
    "        for i, ix in enumerate(neg_samples):\n",
    "            if ix in invalid:\n",
    "                new_ix = -1\n",
    "                while new_ix in invalid:\n",
    "                    new_ix = np.random.choice(self.n, \n",
    "                                              size=1, \n",
    "                                              p=self.distribution)[0]\n",
    "                neg_samples[i] = new_ix\n",
    "        return [int(s) for s in neg_samples]\n",
    "    \n",
    "    def p(self, freqs):\n",
    "        \"\"\"Determine the probability distribution for negative sampling.\n",
    "        \n",
    "        Args:\n",
    "          freqs: List of integers.\n",
    "        \n",
    "        Returns:\n",
    "          numpy.ndarray.\n",
    "        \"\"\"\n",
    "        ### Impelement Me ###\n",
    "        freqs = np.array(freqs)\n",
    "        return np.power(freqs, 3/4) / np.sum(np.power(freqs, 3/4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Prepare Data for the Network\n",
    "\n",
    "Whatever kind of data we pass our neural network, we often need to take some pre-processing steps at training and prediction time. In our case, we need to perform negative sampling and prepare whatever data structure we decide to pass to our network.\n",
    "\n",
    "What is in a batch of data?\n",
    "- center word\n",
    "- context word\n",
    "- negative samples\n",
    "\n",
    "How will we use it?\n",
    "- embedding lookup for all words\n",
    "- dot product of center word with context word and negative samples\n",
    "- softmax over the resulting values\n",
    "\n",
    "We must define our target vector for the cross-entropy loss calculation. Without negative sampling this is a probability distribution over the entire vocabulary with a `1` at the index of the context word. With negative sampling, we only want to calculate loss and backpropagate gradient for the words in each pass: context, negative samples. We can decide how to arrange this. A consistent convention will be convenient. With five negative samples, we will put the context word in the first position, so all our \"targets\" will look like:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We show a row vector here, but in practice this will be a 1D `Tensor`.\n",
    "\n",
    "It will also be convenient to perform all our dot products in parallel. Efficient neural net implementations avoid for loops wherever possible. We can parallelize our implementation by stacking the output embeddings into a matrix and performing a matrix multiplication with the input vector.\n",
    "\n",
    "### To Do\n",
    "\n",
    "Define a `collate(pairs)` function that accepts a `list` of `tuple`s and returns an appropriate data structure, given the considerations just discussed. The tuples will all look like `(center, context)` - i.e. a pair in our `data` variable. The collate function will need to perform negative sampling. It will therefore depend on a NegativeSampler, so we should wrap it and make it a callable.\n",
    "\n",
    "This might be a challenge and you will need to think forward to how to perform the embedding lookup and matrix multiplication.\n",
    "\n",
    "Note: normally we would need to collate labels for training, but since we have a conventional target we don't need to here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Collate:\n",
    "    \n",
    "    def __init__(self, neg_sampler):\n",
    "        self.sampler = neg_sampler\n",
    "    \n",
    "    def __call__(self, pairs):\n",
    "        ### Implement Me ###\n",
    "        batch_size = len(pairs)\n",
    "        centers = [x[0] for x in pairs]\n",
    "        contexts = [x[1] for x in pairs]\n",
    "        context_and_negs = []\n",
    "        for i in range(batch_size):\n",
    "            neg_samples = self.sampler(centers[i])\n",
    "            context_and_negs.append([contexts[i]] + list(neg_samples))\n",
    "        return centers, context_and_negs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will want to create a `DataLoader` with our collate function. This will take care of some nice things for us: \n",
    "- shuffling our training pairs each epoch\n",
    "- facilitating enumeration of batches for training\n",
    "- applying our collate function to each batch\n",
    "- parallelizing this work on the CPU whilst our GPU processes our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data, batch_size, collate_fn):\n",
    "    return dataloader.DataLoader(data, \n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=True, \n",
    "                                 num_workers=1, \n",
    "                                 collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Code the Model\n",
    "\n",
    "For each training pair, we need to:\n",
    "1. Perform embedding lookup for the center word (from $\\mathbf{V}$)\n",
    "2. Lookup the context and negative sample embeddings (from $\\mathbf{U}$)\n",
    "3. Perform a dot product of the center word embeddings with context and negative sample embeddings\n",
    "4. Pass the results of the dot products to the softmax function\n",
    "5. Compute the loss given the labels\n",
    "6. Use the loss to update the vectors\n",
    "\n",
    "Below is a template for the model. We will walk through the steps here. Look for `### Implement Me ###` in the model template for the parts you need to complete. Try not to use PyTorch's built-in functions for softmax and cross-entropy. It is a nice learning exercise to implement these yourself the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    \"\"\"SkipGram Model.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab, emb_dim, num_negs, lr):\n",
    "        \"\"\"Create a new SkipGram.\n",
    "        \n",
    "        Args:\n",
    "          vocab: Dictionary, our vocab dict with token keys and index values.\n",
    "          emb_dim: Integer, the size of word embeddings.\n",
    "          num_negs: Integer, the number of non-context words to sample.\n",
    "          lr: Float, the learning rate for gradient descent.\n",
    "        \"\"\"\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.vocab = vocab\n",
    "        self.n = len(vocab)  # size of the vocab\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_negs = num_negs\n",
    "        \n",
    "        ### Implement Me: define V and U ###\n",
    "        \n",
    "        ### Implement Me: initialize V and U with unform distribution in [-0.01, 0.01] ###\n",
    "        \n",
    "        self.V = nn.Embedding(self.n, emb_dim)\n",
    "        self.U = nn.Embedding(self.n, emb_dim)\n",
    "        nn.init.uniform(self.V.weight, a=-1., b=1.)\n",
    "        nn.init.uniform(self.U.weight, a=-1., b=1.)        \n",
    "        \n",
    "        # Adam is a good optimizer and will converge faster than SGD\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.cuda()\n",
    "    \n",
    "    def forward(self, centers, contexts_negs):\n",
    "        \"\"\"Compute the forward pass of the network.\n",
    "        \n",
    "        Args:\n",
    "          centers: List of integers.\n",
    "          contexts_negs: List of integers\n",
    "        \n",
    "        Returns:\n",
    "          loss (torch.autograd.Variable).\n",
    "        \"\"\"\n",
    "        centers = self.V(self.lookup_tensor(centers)).unsqueeze(1)\n",
    "        contexts_negs = self.U(self.lookup_tensor(contexts_negs)).permute([0, 2, 1])\n",
    "        logits = centers.matmul(contexts_negs).squeeze(1)\n",
    "        predictions = self.softmax(logits)\n",
    "        targets = self.targets(centers.shape[0])\n",
    "        loss = self.loss(predictions, targets)\n",
    "        return loss\n",
    "    \n",
    "    def lookup_tensor(self, indices):\n",
    "        \"\"\"Lookup embeddings given indices.\n",
    "        \n",
    "        Args:\n",
    "          embedding: nn.Parameter, an embedding matrix.\n",
    "          indices: List of integers, the indices to lookup.\n",
    "        \n",
    "        Returns:\n",
    "          torch.autograd.Variable of shape [len(indices), emb_dim]. A matrix \n",
    "            with horizontally stacked word vectors.\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            return Variable(torch.LongTensor(indices),\n",
    "                            requires_grad=False).cuda()\n",
    "        else:\n",
    "            return Variable(torch.LongTensor(indices),\n",
    "                            requires_grad=False)\n",
    "\n",
    "    def loss(self, predictions, targets):\n",
    "        \"\"\"Cross-Entropy Loss.\n",
    "        \n",
    "        Args:\n",
    "          predictions: torch.autograd.Variable (float) of shape (batch_size, num_negs + 1).\n",
    "          targets: torch.autograd.Variable (long) of shape (batch_size, num_negs + 1).\n",
    "        \"\"\"\n",
    "        return -1 * torch.sum(targets * torch.log(predictions))\n",
    "        \n",
    "    def optimize(self, loss):\n",
    "        \"\"\"Optimization step.\n",
    "        \n",
    "        Args:\n",
    "          loss: Scalar.\n",
    "        \"\"\"\n",
    "        # Remove any previous gradient from our tensors before calculating again.\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()        \n",
    "    \n",
    "    def softmax(self, logits):\n",
    "        \"\"\"Softmax function.\n",
    "        \n",
    "        Args:\n",
    "          logits: torch.autograd.Variable of shape (batch_size, num_negs + 1)\n",
    "        \n",
    "        Returns:\n",
    "          torch.autograd.Variable of shape (batch_size, num_negs + 1).\n",
    "        \"\"\"\n",
    "        return torch.exp(logits) / torch.sum(torch.exp(logits))\n",
    "    \n",
    "    def targets(self, batch_size):\n",
    "        \"\"\"Get the conventional targets for the batch.\n",
    "        \n",
    "        Args:\n",
    "          batch_size: Integer.\n",
    "        \n",
    "        Returns:\n",
    "          torch.autograd.Variable (float) of shape (batch_size, num_negs + 1.\n",
    "            the first column is ones the rest are zeros.\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            targets = Variable(torch.zeros((batch_size, self.num_negs + 1)), requires_grad=False).cuda()\n",
    "        else:\n",
    "            targets = Variable(torch.zeros((batch_size, self.num_negs + 1)), requires_grad=False)\n",
    "        targets[:, 0] = 1\n",
    "        return targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Train the Model\n",
    "\n",
    "Training algorithms should consider\n",
    "- Learning rate annealing\n",
    "- Convergence conditions\n",
    "- Early stopping\n",
    "\n",
    "Here we will not anneal the learning rate for simplicity and pre-define an epoch limit.\n",
    "\n",
    "You should experiment with different training strategies in your own work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Step 0\t\tLoss   4.8091\n",
      "Step 1000\t\tLoss   4.9042\n",
      "Step 2000\t\tLoss   4.7576\n",
      "Step 3000\t\tLoss   4.5673\n",
      "Step 4000\t\tLoss   4.4308\n",
      "Step 5000\t\tLoss   4.3357\n",
      "Step 6000\t\tLoss   4.2669\n",
      "Step 7000\t\tLoss   4.2138\n",
      "Step 8000\t\tLoss   4.1744\n",
      "Step 9000\t\tLoss   4.1384\n",
      "Step 10000\t\tLoss   4.1087\n",
      "Step 11000\t\tLoss   4.0863\n",
      "Epoch 2\n",
      "Step 0\t\tLoss   4.0771\n",
      "Step 1000\t\tLoss   4.0502\n",
      "Step 2000\t\tLoss   4.0284\n",
      "Step 3000\t\tLoss   4.0105\n",
      "Step 4000\t\tLoss   3.9963\n",
      "Step 5000\t\tLoss   3.9852\n",
      "Step 6000\t\tLoss   3.9744\n",
      "Step 7000\t\tLoss   3.9661\n",
      "Step 8000\t\tLoss   3.9576\n",
      "Step 9000\t\tLoss   3.9510\n",
      "Step 10000\t\tLoss   3.9451\n",
      "Step 11000\t\tLoss   3.9400\n",
      "Epoch 3\n",
      "Step 0\t\tLoss   3.9376\n",
      "Step 1000\t\tLoss   3.9280\n",
      "Step 2000\t\tLoss   3.9201\n",
      "Step 3000\t\tLoss   3.9139\n",
      "Step 4000\t\tLoss   3.9082\n",
      "Step 5000\t\tLoss   3.9043\n",
      "Step 6000\t\tLoss   3.9016\n",
      "Step 7000\t\tLoss   3.8987\n",
      "Step 8000\t\tLoss   3.8952\n",
      "Step 9000\t\tLoss   3.8930\n",
      "Step 10000\t\tLoss   3.8912\n",
      "Step 11000\t\tLoss   3.8893\n",
      "Epoch 4\n",
      "Step 0\t\tLoss   3.8888\n",
      "Step 1000\t\tLoss   3.8837\n",
      "Step 2000\t\tLoss   3.8792\n",
      "Step 3000\t\tLoss   3.8765\n",
      "Step 4000\t\tLoss   3.8737\n",
      "Step 5000\t\tLoss   3.8713\n",
      "Step 6000\t\tLoss   3.8700\n",
      "Step 7000\t\tLoss   3.8686\n",
      "Step 8000\t\tLoss   3.8669\n",
      "Step 9000\t\tLoss   3.8657\n",
      "Step 10000\t\tLoss   3.8647\n",
      "Step 11000\t\tLoss   3.8639\n",
      "Epoch 5\n",
      "Step 0\t\tLoss   3.8634\n",
      "Step 1000\t\tLoss   3.8598\n",
      "Step 2000\t\tLoss   3.8575\n",
      "Step 3000\t\tLoss   3.8557\n",
      "Step 4000\t\tLoss   3.8544\n",
      "Step 5000\t\tLoss   3.8531\n",
      "Step 6000\t\tLoss   3.8522\n",
      "Step 7000\t\tLoss   3.8517\n",
      "Step 8000\t\tLoss   3.8510\n",
      "Step 9000\t\tLoss   3.8499\n",
      "Step 10000\t\tLoss   3.8492\n",
      "Step 11000\t\tLoss   3.8488\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters (play with these yourself)\n",
    "max_epochs = 5\n",
    "emb_dim = 10\n",
    "num_negs = 5\n",
    "lr = 0.01\n",
    "batch_size = 16\n",
    "\n",
    "sampler = NegativeSampler(vocab, frequencies, contexts, num_negs)\n",
    "collate = Collate(sampler)\n",
    "data_loader = get_data_loader(list(pairs), batch_size, collate_fn=collate)\n",
    "skipgram = SkipGram(vocab, emb_dim, num_negs, lr)\n",
    "if torch.cuda.is_available():\n",
    "    skipgram.cuda()\n",
    "\n",
    "epoch = 0\n",
    "global_step = 0\n",
    "cum_loss = 0.\n",
    "while epoch < max_epochs:\n",
    "    epoch += 1\n",
    "    print('Epoch %s' % epoch)\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        global_step  += 1\n",
    "        loss = skipgram.forward(*batch)\n",
    "        skipgram.optimize(loss)\n",
    "        loss = loss.data.cpu().numpy()[0]\n",
    "        cum_loss += loss\n",
    "        if step % 1000 == 0:\n",
    "            print('Step %s\\t\\tLoss %8.4f' % (step, cum_loss / (global_step * batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the average accumulated loss steadily decrease and start to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Embeddings\n",
    "\n",
    "We use TSNE to learn a 2D representation of our vectors. Using TSNE can be sensitive to hyperparemter selection (see https://distill.pub/2016/misread-tsne/). But we should get a bit of a feel for what has been learned.\n",
    "\n",
    "We pick a few words we expect to see close or far from each other, and a few other random words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5154f2dac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "choices = ['man', 'woman', 'queen', 'king', \n",
    "           'Jewish', 'massacre', 'holocaust', \n",
    "           'Globalization', 'Malthusian', 'Privatization', 'immigration', 'Economics', \n",
    "           'selfish', 'selfless' ,\n",
    "           'epidemic', 'harming', 'combat']\n",
    "choice_ixs = [vocab[c] for c in choices]\n",
    "random_ixs = [int(x) for x in np.random.choice(range(len(vocab)), 10)]\n",
    "tok_ixs = choice_ixs + random_ixs\n",
    "lookup_ixs = torch.LongTensor(tok_ixs)\n",
    "embeddings = skipgram.V.weight[lookup_ixs].data.cpu().numpy()\n",
    "tsne = TSNE(n_components=2, perplexity=2)\n",
    "X_tsne = tsne.fit_transform(embeddings)\n",
    "df = pd.DataFrame(X_tsne, index={rev_vocab[t]: t for t in tok_ixs}, columns=['x', 'y'])\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(df['x'], df['y'])\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'selfish' in vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_py35)",
   "language": "python",
   "name": "conda_py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
